<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>    <link>https://s3alfisc.github.io/blog/categories/ci/</link>
    <description>Recent blog posts by </description>    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 18 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://s3alfisc.github.io/blog/categories/ci/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Testing an R package against a Julia package on github actions</title>
      <link>https://s3alfisc.github.io/blog/post/testing-an-r-package-against-a-julia-package-via-github-actions/</link>
      <pubDate>Sat, 18 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://s3alfisc.github.io/blog/post/testing-an-r-package-against-a-julia-package-via-github-actions/</guid>
   <description>
&lt;script src=&#34;https://s3alfisc.github.io/blog/post/testing-an-r-package-against-a-julia-package-via-github-actions/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Something I have learned the hard way: every single line of code that I write should be accompanied by a unit test. Unit tests are not only useful for me as a developer to spot bugs, but a thorough sequence of unit tests should also increase others’ confidence in the quality of my code.&lt;/p&gt;
&lt;p&gt;For statistical methods and algorithms, one great way to test if everything is working as intended is to test one’s own implementation of an algorithm against someone else’s.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:pressure&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;Caravaggio_Sacrifice_of_Isaac.jpg&#34; alt=&#34;Abraham is *severely* tested. (Sacrifice of Isaac, Caravaggio, ca. 1598. Barbara Piasecka-Johnson Collection.)&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Abraham is &lt;em&gt;severely&lt;/em&gt; tested. (Sacrifice of Isaac, Caravaggio, ca. 1598. Barbara Piasecka-Johnson Collection.)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This idea is expressed in &lt;a href=&#34;https://stats-devguide.ropensci.org/standards.html&#34;&gt;ropensci’s statistical software standards&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;G5.4 Correctness tests to test that statistical algorithms produce expected results to some fixed test data sets (potentially through comparisons using binding frameworks such as RStata).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;G5.4b For new implementations of existing methods, correctness tests should include tests against previous implementations. Such testing may explicitly call those implementations in testing, preferably from fixed-versions of other software, or use stored outputs from those where that is not possible.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Preferably, these unit tests should be automized via continuous integration tools such as github actions. CI is super useful - a CI workflow might e.g. trigger to run all tests at every commit, so I &lt;em&gt;definitely&lt;/em&gt; won’t forget to run them. Second, CI integration via github actions communicates to my package’s user that I have actually and successfully run all unit tests on the latest development version. Last, all tests are run “in the cloud”, so I am not blocking my own computer for (potentially) multiple minutes.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As expressed in ropensci’s guideline G5.4b, in scientific computing, the same algorithm might only be implemented just once, or, in the lucky case that another implementation exists, it might be written in another language.&lt;/p&gt;
&lt;p&gt;This applies to the “fast” cluster bootstrap algorithm implemented in &lt;a href=&#34;https://github.com/s3alfisc/fwildclusterboot&#34;&gt;fwildclusterboot&lt;/a&gt;. To my knowledge, there exists an R implementation (&lt;code&gt;fwildclusterboot&lt;/code&gt;), the original Stata version (&lt;a href=&#34;https://github.com/droodman/boottest&#34;&gt;boottest&lt;/a&gt;), and recently, David Roodman has written a Julia package, &lt;a href=&#34;https://github.com/droodman/WildBootTests.jl&#34;&gt;WildBootTests.jl&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Up until now, &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; test sequence has been divided in two separate parts. Part one’s job was to test if the methods for different regression packages in R producec consistent results (“internal consistency”). These tests could easily be run on CRAN and github actions, as they only required &lt;code&gt;fwildclusterboot&lt;/code&gt; and a working R distribution.&lt;/p&gt;
&lt;p&gt;Part two of the test sequence tested &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; “external validity” by running the bootstrap in both R and Stata (via &lt;a href=&#34;https://github.com/droodman/boottest&#34;&gt;boottest&lt;/a&gt; &amp;amp; the RStata package). Before a CRAN submission, I would run all these tests on my local machine. Of course, the unit tests involving Stata could not be run on CRAN, and I did not think (and am now uncertain) that these tests could be automatized via github actions.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As it happend, my Stata student license expired the other week, and when I checked StataCorps website for renewal fees, Stata - for non-academics - had a &lt;a href=&#34;https://dpc-onlineshop.de/epages/1188aeca-9275-49f6-be7a-ac117311e6ae.sf/de_DE/?ObjectPath=/Shops/1188aeca-9275-49f6-be7a-ac117311e6ae/Products/5170113&#34;&gt;840 € price tag for a one-year license&lt;/a&gt;. To me, this seemed to be a hefty price tag for running a few unit tests for &lt;code&gt;fwildclusterboot&lt;/code&gt;, and was a good incentive to update my unit tests.&lt;/p&gt;
&lt;p&gt;Luckily, &lt;code&gt;WildBootTests.jl&lt;/code&gt; is now around, and as Julia is open source, I was fairly certain that I could transfer my “external” tests from Stata to Julia and deploy all tests on github actions.&lt;/p&gt;
&lt;div id=&#34;how-to-set-up-a-workflow-that-tests-an-r-package-against-a-julia-package&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to set up a workflow that tests an R package against a Julia package&lt;/h3&gt;
&lt;p&gt;So, how does a workflow to test an R package against a Julia package looks like?&lt;/p&gt;
&lt;p&gt;First of all, I have used the skeleton of &lt;code&gt;fwildclusterboot&lt;/code&gt; to produce a wrapper around &lt;code&gt;WildBootTests.jl&lt;/code&gt;, &lt;a href=&#34;https://github.com/s3alfisc/wildboottestjlr&#34;&gt;wildboottestjlr&lt;/a&gt;. All data pre-processing is handled by functions copied from &lt;code&gt;fwildclusterboot&lt;/code&gt;. For running the bootstrap, R sends all required objects to Julia (via the excellent &lt;code&gt;JuliaConnectoR&lt;/code&gt; package, on which I will blog in the future). In a second step, I have added &lt;code&gt;wildboottestjlr&lt;/code&gt; to &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; dependencies and have initiated a github actions workflow via &lt;code&gt;usethis::use_github_actions()&lt;/code&gt;. (Note that it is in fact not neccessariy to write an entire R wrapper-package for the Julia algorithm you want to test. &lt;code&gt;wildboottestjlr&lt;/code&gt; exists because &lt;code&gt;WildBootTests.jl&lt;/code&gt; is actually a magnitude faster than &lt;code&gt;fwildclusterboot&lt;/code&gt; and because it has many additional feature, as e.g. a wild cluster bootstrap for instrumental variables.)&lt;/p&gt;
&lt;p&gt;The created workflow yaml looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For help debugging build failures open an issue on the RStudio community with the &amp;#39;github-actions&amp;#39; tag.
# https://community.rstudio.com/new-topic?category=Package%20development&amp;amp;tags=github-actions
on:
  push:
    branches:
      - main
      - master
  pull_request:
    branches:
      - main
      - master

name: R-CMD-check

jobs:
  R-CMD-check:
    runs-on: ${{ matrix.config.os }}

    name: ${{ matrix.config.os }} (${{ matrix.config.r }})

    strategy:
      fail-fast: false
      matrix:
        config:
          - {os: windows-latest, r: &amp;#39;release&amp;#39;}
          - {os: macOS-latest, r: &amp;#39;release&amp;#39;}
          - {os: ubuntu-20.04, r: &amp;#39;release&amp;#39;, rspm: &amp;quot;https://packagemanager.rstudio.com/cran/__linux__/focal/latest&amp;quot;}

    env:
      R_REMOTES_NO_ERRORS_FROM_WARNINGS: true
      RSPM: ${{ matrix.config.rspm }}
      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}

    steps:
      - uses: actions/checkout@v2

      - uses: r-lib/actions/setup-r@v2
        with:
          r-version: ${{ matrix.config.r }}

      - uses: r-lib/actions/setup-pandoc@v2

      - name: Query dependencies
        run: |
          install.packages(&amp;#39;remotes&amp;#39;)
          saveRDS(remotes::dev_package_deps(dependencies = TRUE), &amp;quot;.github/depends.Rds&amp;quot;, version = 2)
          writeLines(sprintf(&amp;quot;R-%i.%i&amp;quot;, getRversion()$major, getRversion()$minor), &amp;quot;.github/R-version&amp;quot;)
        shell: Rscript {0}

      - name: Cache R packages
        if: runner.os != &amp;#39;Windows&amp;#39;
        uses: actions/cache@v2
        with:
          path: ${{ env.R_LIBS_USER }}
          key: ${{ runner.os }}-${{ hashFiles(&amp;#39;.github/R-version&amp;#39;) }}-1-${{ hashFiles(&amp;#39;.github/depends.Rds&amp;#39;) }}
          restore-keys: ${{ runner.os }}-${{ hashFiles(&amp;#39;.github/R-version&amp;#39;) }}-1-

      - name: Install system dependencies
        if: runner.os == &amp;#39;Linux&amp;#39;
        run: |
          while read -r cmd
          do
            eval sudo $cmd
          done &amp;lt; &amp;lt;(Rscript -e &amp;#39;writeLines(remotes::system_requirements(&amp;quot;ubuntu&amp;quot;, &amp;quot;20.04&amp;quot;))&amp;#39;)
      - name: Install dependencies
        run: |
          remotes::install_deps(dependencies = TRUE)
          remotes::install_cran(&amp;quot;rcmdcheck&amp;quot;)
        shell: Rscript {0}
        
      # install julia
      - uses: julia-actions/setup-julia@v1
      # add julia to renviron
      - name: Create and populate .Renviron file
        run: echo JULIA_BINDIR= &amp;quot;${{ env.juliaLocation }}&amp;quot; &amp;gt;&amp;gt; ~/.Renviron
        shell: bash

      # install WildBootTests.jl
      - name: install WildBootTests.jl 
        run: julia -e &amp;#39;using Pkg; Pkg.add(&amp;quot;WildBootTests&amp;quot;)&amp;#39;
        # use shell bash to ensure consistent behavior across OS
        shell: bash

      - name: Check
        env:
          _R_CHECK_CRAN_INCOMING_REMOTE_: false
        run: rcmdcheck::rcmdcheck(args = c(&amp;quot;--no-manual&amp;quot;, &amp;quot;--as-cran&amp;quot;), error_on = &amp;quot;warning&amp;quot;, check_dir = &amp;quot;check&amp;quot;)
        shell: Rscript {0}

      - name: Upload check results
        if: failure()
        uses: actions/upload-artifact@main
        with:
          name: ${{ runner.os }}-r${{ matrix.config.r }}-results
          path: check&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a nutshell, this workflow installs R and &lt;code&gt;fwildclusterboot&lt;/code&gt; in a clean ubuntu environment, conducts a cmd-check and initiates all of &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; unit tests.&lt;/p&gt;
&lt;p&gt;To enable testing against Julia and &lt;code&gt;WildBootTests.jl&lt;/code&gt;, it further adds three ‘steps’ to the workflow: the workflow has to install &lt;code&gt;Julia&lt;/code&gt;, &lt;code&gt;WildBootTests.jl&lt;/code&gt; (and all its Julia dependencies) and link R and Julia so that &lt;code&gt;JuliaConnectoR&lt;/code&gt; could communicate between the two languages.&lt;/p&gt;
&lt;p&gt;This is easily be achieved by adding the lines below to the yaml file create by &lt;code&gt;usethis::use_github_action()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install julia
- uses: julia-actions/setup-julia@v1
# add julia to renviron
- name: Create and populate .Renviron file
  run: echo JULIA_BINDIR= &amp;quot;${{ env.juliaLocation }}&amp;quot; &amp;gt;&amp;gt; ~/.Renviron
  shell: bash

# install WildBootTests.jl
  - name: install WildBootTests.jl 
    run: julia -e &amp;#39;using Pkg; Pkg.add(&amp;quot;WildBootTests&amp;quot;)&amp;#39;
  # use shell bash to ensure consistent behavior across OS
    shell: bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Placing these few lines in the workflow yaml before the cmd check is triggered completes the workflow. It now downloads R and Julia, installs &lt;code&gt;fwildclusterboot&lt;/code&gt;, &lt;code&gt;wildboottestjlr&lt;/code&gt; and &lt;code&gt;WildBootTests.jl&lt;/code&gt;, links R and Julia, and runs all of &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; unit tests. The remaining step is to push to github, and then all unit tests are running!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The unit tests for fwildclusterboot currently run for about 30 minutes.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I have not implemented test that compare &lt;code&gt;fwildclusterboot&lt;/code&gt; against other R implementations of the wild bootstrap, mainly due to performance reasons. As resampling methods are usually time intensive and fwildclusterboot is &lt;a href=&#34;https://s3alfisc.github.io/blog/post/1000x-faster-wild-cluster-bootstrap-inference-in-r-with-fwildclusterboot/&#34;&gt;much faster&lt;/a&gt; than other R implementations, running unit tests against other R implementations would simply take a &lt;em&gt;very&lt;/em&gt; long time.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>    </item>
    
  </channel>
</rss>
