<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>    <link>https://s3alfisc.github.io/blog/categories/multiple-testing/</link>
    <description>Recent blog posts by </description>    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 25 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://s3alfisc.github.io/blog/categories/multiple-testing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Fast Romano-Wolf Multiple Testing Corrections for fixest üê∫</title>
      <link>https://s3alfisc.github.io/blog/post/2023-01-25-fast-romano-wolf-multiple-testing-corrections-for-fixest/</link>
      <pubDate>Wed, 25 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://s3alfisc.github.io/blog/post/2023-01-25-fast-romano-wolf-multiple-testing-corrections-for-fixest/</guid>
   <description>


&lt;p&gt;For the final chapter of my dissertation, I had examined the effects of ordinal class rank on the academic achievement of Danish primary school students (following a popular identification strategy introduced in a paper by Murphy and Weinhard). Because of the richness of the Danish register data, I had a large number of potential outcome variables at my disposal, and as a result, I was able to examine literally all the outcomes that the previous literature had covered in individual studies - the effect of rank on achievement, personality, risky behaviour, mental health, parental investment, etc. - in one paper.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;rank-mechanisms.png&#34; alt=&#34;The Effect of Ordinal Class Rank on quite a few outcome variables&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: The Effect of Ordinal Class Rank on quite a few outcome variables
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;But with (too) many outcome variables comes a risk: inflated type 1 error rates, or an increased risk of ‚Äòfalse positives‚Äô. So I was wondering: were all the significant effects I estimated - shown in the figure above - ‚Äúreal‚Äù, or was I simply being fooled by randomness?&lt;/p&gt;
&lt;p&gt;A common way to control the risk of false positive when testing multiple hypothesis is to use methods that control the ‚Äòfamily-wise‚Äô error rate, i.e.¬†the risk of at least one false positive in a family of S hypotheses.&lt;/p&gt;
&lt;p&gt;Among such methods, Romano and Wolf‚Äôs correction is particularly popular, because it is ‚Äúuniformly the most powerful‚Äù. Without going into too much detail, I‚Äôll just say that if you have a choice between a number of methods that control the family-wise error rate at a desired level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, you might want to choose the ‚Äúmost powerful‚Äù one, i.e.¬†the one that has the highest success rate of finding a true effect.&lt;/p&gt;
&lt;p&gt;Now, there is actually an amazing Stata package for the Romano-Wolf method called &lt;code&gt;rwolf&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But this is an R blog, and I‚Äôm an R guy ‚Ä¶ In addition, my regression involved several million rows and some high-dimensional fixed effects, and &lt;code&gt;rwolf&lt;/code&gt; quickly showed its limitations. It just wasn‚Äôt fast enough!&lt;/p&gt;
&lt;p&gt;While playing around with the &lt;code&gt;rwolf&lt;/code&gt; package, I finally did my due diligence on the method it implements, and after a little background reading, I realized that both the Romano and Wolf method - as well as its main rival, the method proposed by Westfall and Young - are based on the bootstrap!&lt;/p&gt;
&lt;p&gt;But wait! Had I not just spent a lot of time &lt;a href=&#34;https://s3alfisc.github.io/blog/post/1000x-faster-wild-cluster-bootstrap-inference-in-r-with-fwildclusterboot/&#34;&gt;porting a super-fast bootstrap algorithm from R to Stata&lt;/a&gt;? Could I not use Roodman et al‚Äôs ‚Äúfast and wild‚Äù cluster bootstrap algorithm for bootstrap-based multiple hypothesis correction?&lt;/p&gt;
&lt;p&gt;Of course it was inevitable: I ended up writing an R package. Today I am happy to present the first MVP version of the &lt;code&gt;wildwrwolf&lt;/code&gt; package!&lt;/p&gt;
&lt;div id=&#34;the-wildrwolf-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The wildrwolf package&lt;/h2&gt;
&lt;p&gt;You can simply install the package from github or r-universe by typing&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install.packages(&amp;quot;devtools&amp;quot;)
devtools::install_github(&amp;quot;s3alfisc/wildrwolf&amp;quot;)

# from r-universe (windows &amp;amp; mac, compiled R &amp;gt; 4.0 required)
install.packages(&amp;#39;wildrwolf&amp;#39;, repos =&amp;#39;https://s3alfisc.r-universe.dev&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Romano Wolf correction in &lt;code&gt;wildrwolf&lt;/code&gt; is implemented as a post-estimation commands for multiple estimation objects from the fabulous &lt;code&gt;fixest&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;To demonstrate how &lt;code&gt;wildrwolf&#39;s&lt;/code&gt; main function, &lt;code&gt;rwolf&lt;/code&gt;, works, let‚Äôs first simulate some data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(wildrwolf)
library(fixest)

set.seed(1412)

library(wildrwolf)
library(fixest)

set.seed(1412)

N &amp;lt;- 5000
X1 &amp;lt;- rnorm(N)
X2 &amp;lt;- rnorm(N)
rho &amp;lt;- 0.5
sigma &amp;lt;- matrix(rho, 4, 4); diag(sigma) &amp;lt;- 1
u &amp;lt;- MASS::mvrnorm(n = N, mu = rep(0, 4), Sigma = sigma)
Y1 &amp;lt;- 1 + 1 * X1 + X2 
Y2 &amp;lt;- 1 + 0.01 * X1 + X2
Y3 &amp;lt;- 1 + 0.4 * X1 + X2
Y4 &amp;lt;- 1 + -0.02 * X1 + X2
for(x in 1:4){
  var_char &amp;lt;- paste0(&amp;quot;Y&amp;quot;, x)
  assign(var_char, get(var_char) + u[,x])
}
group_id &amp;lt;- sample(1:100, N, TRUE)

data &amp;lt;- data.frame(Y1 = Y1,
                   Y2 = Y2,
                   Y3 = Y3,
                   Y4 = Y4,
                   X1 = X1,
                   X2 = X2,
                   group_id = group_id,
                   splitvar = sample(1:2, N, TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now estimate a regression via the &lt;code&gt;fixest&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- feols(c(Y1, Y2, Y3, Y4) ~ csw(X1,X2),
             data = data,
             cluster = ~group_id,
             ssc = ssc(cluster.adj = TRUE))

# clean workspace except for res &amp;amp; data
rm(list= ls()[!(ls() %in% c(&amp;#39;fit&amp;#39;,&amp;#39;data&amp;#39;))])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For all eight estimated regressions, we want to apply the Romano-Wolf correction to the parameter of interest, &lt;code&gt;X1&lt;/code&gt;. We simply need to provide the regression object of type &lt;code&gt;fixest_multi&lt;/code&gt; (it is also possible to simply provide a list of objects of type &lt;code&gt;fixest&lt;/code&gt;), the parameter of interest, the number of bootstrap draws, and possibly a seed (for replicability). That‚Äôs it! If the regressions use clustered standard errors, &lt;code&gt;rwolf&lt;/code&gt; will pick this up and run a wild cluster bootstrap, otherwise just a robust wild bootstrap.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pracma::tic()
res_rwolf &amp;lt;- wildrwolf::rwolf(
  models = fit,
  param = &amp;quot;X1&amp;quot;, 
  B = 9999, 
  seed = 23
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |=========                                                             |  12%
  |                                                                            
  |==================                                                    |  25%
  |                                                                            
  |==========================                                            |  38%
  |                                                                            
  |===================================                                   |  50%
  |                                                                            
  |============================================                          |  62%
  |                                                                            
  |====================================================                  |  75%
  |                                                                            
  |=============================================================         |  88%
  |                                                                            
  |======================================================================| 100%&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pracma::toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## elapsed time is 3.980000 seconds&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(N=5000\)&lt;/span&gt; observations with &lt;span class=&#34;math inline&#34;&gt;\(G=100\)&lt;/span&gt; clusters, &lt;span class=&#34;math inline&#34;&gt;\(S=8\)&lt;/span&gt; hypothesis and &lt;span class=&#34;math inline&#34;&gt;\(B=9999\)&lt;/span&gt; bootstrap draws, the calculation of Romano-Wolf corrected p-values takes less than 5 seconds. If you ask me, that is pretty fast! =) üöÄ&lt;/p&gt;
&lt;p&gt;We can now compare the results of &lt;code&gt;rwolf&lt;/code&gt; with the uncorrected p-values and another popular multiple testing adjustment, Holm‚Äôs method.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pvals &amp;lt;- lapply(fit, function(x) pvalue(x)[&amp;quot;X1&amp;quot;]) |&amp;gt; unlist()
  
df &amp;lt;- 
  data.frame(
    &amp;quot;uncorrected&amp;quot; = pvals, 
  &amp;quot;Holm&amp;quot; = p.adjust(pvals, method = &amp;quot;holm&amp;quot;), 
  &amp;quot;rwolf&amp;quot; = res_rwolf$pval
)
rownames(df) &amp;lt;- NULL
round(df, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   uncorrected  Holm rwolf
## 1       0.000 0.000 0.000
## 2       0.000 0.000 0.000
## 3       0.140 0.420 0.367
## 4       0.033 0.132 0.128
## 5       0.000 0.000 0.000
## 6       0.000 0.000 0.000
## 7       0.398 0.420 0.394
## 8       0.152 0.420 0.367&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both Holm‚Äôs method and &lt;code&gt;rwolf&lt;/code&gt; produce similar corrected p-values, which - in general - are larger than the uncorrected p-values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;but-does-it-actually-work-monte-carlo-experiments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;But does it actually work? Monte Carlo Experiments&lt;/h2&gt;
&lt;p&gt;We test &lt;span class=&#34;math inline&#34;&gt;\(S=6\)&lt;/span&gt; hypotheses and generate data as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_{i,s,g} = \beta_{0} + \beta_{1,s} D_{i} + u_{i,g} + \epsilon_{i,s} \]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(D_i = 1(U_i &amp;gt; 0.5)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U_i\)&lt;/span&gt; is drawn from a uniform
distribution, &lt;span class=&#34;math inline&#34;&gt;\(u_{i,g}\)&lt;/span&gt; is a cluster level shock with intra-cluster
correlation &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt;, and the idiosyncratic error term is drawn from a
multivariate random normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(0_S\)&lt;/span&gt; and covariance
matrix&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;S &amp;lt;- 6
rho &amp;lt;- 0.5
Sigma &amp;lt;- matrix(1, 6, 6)
diag(Sigma) &amp;lt;- rho
Sigma&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This experiment imposes a data generating process as in equation (9) in
&lt;a href=&#34;https://docs.iza.org/dp12845.pdf&#34;&gt;Clarke, Romano and Wolf&lt;/a&gt;, with an
additional error term &lt;span class=&#34;math inline&#34;&gt;\(u_g\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(G=20\)&lt;/span&gt; clusters and intra-cluster
correlation 0.5 and &lt;span class=&#34;math inline&#34;&gt;\(N=1000\)&lt;/span&gt; observations.&lt;/p&gt;
&lt;p&gt;You can run the simulations via the &lt;code&gt;run_fwer_sim()&lt;/code&gt; function attached
in the package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# note that this will take some time
res &amp;lt;- run_fwer_sim(
  seed = 232123,
  n_sims = 500,
  B = 499,
  N = 1000,
  s = 6, 
  rho = 0.5 #correlation between hypotheses, not intra-cluster!
)

# &amp;gt; res
#                 reject_5 reject_10 rho
# fit_pvalue         0.274     0.460 0.5
# fit_pvalue_holm    0.046     0.112 0.5
# fit_padjust_rw     0.052     0.110 0.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both Holm‚Äôs method and &lt;code&gt;wildrwolf&lt;/code&gt; control the family wise error rates, at both the 5 and 10% significance level. Very cool!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-method-by-westfall-and-young&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The method by Westfall and Young&lt;/h2&gt;
&lt;p&gt;A package for Westfall and Young‚Äôs (1993) method is currently &lt;a href=&#34;https://github.com/s3alfisc/wildwyoung&#34;&gt;in development&lt;/a&gt;. I am not yet fully convinced that it works as intented - in simulations, I regularly find that &lt;code&gt;wildwyoung&lt;/code&gt; overrejects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;literature&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Literature&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Clarke, Damian, Joseph P. Romano, and Michael Wolf. ‚ÄúThe Romano‚ÄìWolf multiple-hypothesis correction in Stata.‚Äù The Stata Journal 20.4 (2020): 812-843.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Murphy, Richard, and Felix Weinhardt. ‚ÄúTop of the class: The importance of ordinal rank.‚Äù The Review of Economic Studies 87.6 (2020): 2777-2826.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Romano, Joseph P., and Michael Wolf. ‚ÄúStepwise multiple testing as formalized data snooping.‚Äù Econometrica 73.4 (2005): 1237-1282.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Roodman, David, et al.¬†‚ÄúFast and wild: Bootstrap inference in Stata using boottest.‚Äù The Stata Journal 19.1 (2019): 4-60.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Westfall, Peter H., and S. Stanley Young. Resampling-based multiple testing: Examples and methods for p-value adjustment. Vol. 279. John Wiley &amp;amp; Sons, 1993.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>    </item>
    
  </channel>
</rss>
