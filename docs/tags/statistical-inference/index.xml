<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>    <link>https://s3alfisc.github.io/blog/tags/statistical-inference/</link>
    <description>Recent blog posts by </description>    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 06 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://s3alfisc.github.io/blog/tags/statistical-inference/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Fast Wild Cluster Bootstrapping in Python via wildboottest üêç</title>
      <link>https://s3alfisc.github.io/blog/post/2022-12-11-fast-wild-cluster-bootstrap-inference-in-python-with-wildboottest/</link>
      <pubDate>Sun, 06 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://s3alfisc.github.io/blog/post/2022-12-11-fast-wild-cluster-bootstrap-inference-in-python-with-wildboottest/</guid>
   <description>


&lt;p&gt;&lt;a href=&#34;https://amichuda.github.io/&#34;&gt;Aleksandr Michuda&lt;/a&gt; and I have just released version 0.1 of &lt;a href=&#34;https://github.com/s3alfisc/wildboottest&#34;&gt;&lt;code&gt;wildboottest&lt;/code&gt;&lt;/a&gt; to &lt;a href=&#34;https://pypi.org/project/wildboottest/&#34;&gt;PyPi&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;wildboottest&lt;/code&gt; is a Python package to conduct &lt;em&gt;fast&lt;/em&gt; wild cluster bootstrap inference in Python and implements the wild cluster bootstrap following algorithms sketched out in &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S2452306221001404&#34;&gt;MacKinnon (2021)&lt;/a&gt; and &lt;a href=&#34;https://www.econ.queensu.ca/sites/econ.queensu.ca/files/wpaper/qed_wp_1485.pdf&#34;&gt;MacKinnon, Nielsen &amp;amp; Webb, 2022 (MNW)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Most importantly, it supports all eight variants of the wild cluster bootstrap discussed in MNW as well as CRV3 inference via the cluster jackknife. Some of these new variants appear to perform even better than the ‚Äústandard‚Äù (WCR11) wild cluster bootstrap in situations where the textbook CRV1 cluster robust variance estimator is known to struggle. And thanks to the excellent &lt;a href=&#34;https://github.com/numba/numba&#34;&gt;&lt;code&gt;numba&lt;/code&gt;&lt;/a&gt; library, it is actually quite fast!&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;MNW2022.png&#34; alt=&#34;Rejection Frequencies of different Wild Cluster Bootstrap Variants (Figure from MNW (2022, full citation below). The main takeaway is that the new bootstrap variants appear to perform really, really well!&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Rejection Frequencies of different Wild Cluster Bootstrap Variants (Figure from MNW (2022, full citation below). The main takeaway is that the new bootstrap variants appear to perform really, really well!
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In terms of functionality, &lt;code&gt;wildboottest&lt;/code&gt; still lacks behind its sister packages (Stata‚Äôs &lt;a href=&#34;https://github.com/droodman/boottest&#34;&gt;boottest&lt;/a&gt;, R‚Äôs &lt;a href=&#34;https://github.com/s3alfisc/fwildclusterboot&#34;&gt;fwildcusterboot&lt;/a&gt; and Julia‚Äôs &lt;a href=&#34;https://github.com/droodman/WildBootTests.jl&#34;&gt;WildBootTests.jl&lt;/a&gt;). &lt;code&gt;wildboottest&lt;/code&gt; supports&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The wild cluster bootstrap for OLS (&lt;a href=&#34;https://direct.mit.edu/rest/article-abstract/90/3/414/57731/Bootstrap-Based-Improvements-for-Inference-with&#34;&gt;Cameron, Gelbach &amp;amp; Miller 2008&lt;/a&gt;, &lt;a href=&#34;https://econpapers.repec.org/paper/qedwpaper/1406.htm&#34;&gt;Roodman et al (2019)&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Multiple new versions of the wild cluster bootstrap as described in &lt;a href=&#34;https://www.econ.queensu.ca/sites/econ.queensu.ca/files/wpaper/qed_wp_1485.pdf&#34;&gt;MacKinnon, Nielsen &amp;amp; Webb (2022)&lt;/a&gt;, including the WCR13, WCR31, WCR33, WCU13, WCU31 and WCU33.&lt;/li&gt;
&lt;li&gt;CRV1 and CRV3 robust variance estimation, including the CRV3-Jackknife as described in &lt;a href=&#34;https://arxiv.org/pdf/2205.03288.pdf&#34;&gt;MacKinnon, Nielsen &amp;amp; Webb (2022)&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Features that are currently not (yet) supported:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The (non-clustered) wild bootstrap for OLS (&lt;a href=&#34;https://projecteuclid.org/journals/annals-of-statistics/volume-14/issue-4/Jackknife-Bootstrap-and-Other-Resampling-Methods-in-Regression-Analysis/10.1214/aos/1176350142.full&#34;&gt;Wu, 1986&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;The subcluster bootstrap (&lt;a href=&#34;https://academic.oup.com/ectj/article-abstract/21/2/114/5078969?login=false&#34;&gt;MacKinnon and Webb 2018&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Confidence intervals formed by inverting the test and iteratively searching for bounds.&lt;/li&gt;
&lt;li&gt;Multiway clustering.&lt;/li&gt;
&lt;li&gt;Regression Weights (Weighted Least Squares / WLS).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can install the package from PyPi by running&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;pip install wildboottest&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here‚Äôs a small example on how to use &lt;code&gt;wildboottest&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from wildboottest.wildboottest import wildboottest
import statsmodels.api as sm
import numpy as np
import pandas as pd

# create data
np.random.seed(12312312)
N = 1000
k = 10
G = 25
X = np.random.normal(0, 1, N * k).reshape((N,k))
X = pd.DataFrame(X)
X.rename(columns = {0:&amp;quot;X1&amp;quot;}, inplace = True)
beta = np.random.normal(0,1,k)
beta[0] = 0.005
u = np.random.normal(0,1,N)
Y = 1 + X @ beta + u
cluster = np.random.choice(list(range(0,G)), N)

# estimation
model = sm.OLS(Y, X)

wildboottest(model, param = &amp;quot;X1&amp;quot;, cluster = cluster, B = 9999, bootstrap_type = &amp;quot;11&amp;quot;)
#   param              statistic   p-value
# 0    X1  [-1.0530803154504016]  0.308831

wildboottest(model, param = &amp;quot;X1&amp;quot;, cluster = cluster, B = 9999, bootstrap_type = &amp;quot;31&amp;quot;)
#   param              statistic   p-value
# 0    X1  [-1.0530803154504016]  0.307631

wildboottest(model, param = &amp;quot;X1&amp;quot;, cluster = cluster, B = 9999, bootstrap_type = &amp;quot;33&amp;quot;)
#   param              statistic   p-value
# 0    X1  [-1.0394791020434824]  0.294286&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This was the first time I have worked on a Python package, and it has been quite a nice experience - after having used quite a bit of Python at work, I have now actually started to enjoy Python and object oriented programming! The wild cluster bootstrap variants really fit nicely into an OOP framework, and I am really impressed by the &lt;code&gt;numba&lt;/code&gt; jit compiler. Submitting to PyPi was a surprisingly smooth experience as wellüòÑ.&lt;/p&gt;
&lt;p&gt;What are the next steps for &lt;code&gt;wildboottest&lt;/code&gt;? We need to close a few performance bottlenecks, in particular for the WCRx3 bootstrap types, and then I‚Äôd like to close the functionality gaps discussed above. I‚Äôd also like to allow users to call &lt;code&gt;WildBootTests.jl&lt;/code&gt;, which is just blazing fast. And optimally, we‚Äôll make the package callable from &lt;code&gt;statsmodels&lt;/code&gt; and &lt;code&gt;linearmodels&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;And no, despite having a lot of fun working on &lt;code&gt;wildboottest&lt;/code&gt; and some recent troubles of getting &lt;code&gt;fwildclusterboot&lt;/code&gt; back to CRAN, I don‚Äôt plan to stop developing in R üòÑ&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MacKinnon - Fast cluster bootstrap methods for linear regression models, 2021, &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S2452306221001404&#34;&gt;Econometrics &amp;amp; Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MacKinnon, Nielsen &amp;amp; Webb - Fast and Reliable Jackknife and Bootstrap Methods for Cluster-Robust Inference, 2022, &lt;a href=&#34;https://www.econ.queensu.ca/sites/econ.queensu.ca/files/wpaper/qed_wp_1485.pdf&#34;&gt;Queens University Working Paper No 1485&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>    </item>
    
    <item>
      <title>1000x faster Wild Cluster Bootstrap Inference in R with fwildclusterboot üöÄ</title>
      <link>https://s3alfisc.github.io/blog/post/1000x-faster-wild-cluster-bootstrap-inference-in-r-with-fwildclusterboot/</link>
      <pubDate>Sun, 12 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://s3alfisc.github.io/blog/post/1000x-faster-wild-cluster-bootstrap-inference-in-r-with-fwildclusterboot/</guid>
   <description>
&lt;script src=&#34;https://s3alfisc.github.io/blog/post/1000x-faster-wild-cluster-bootstrap-inference-in-r-with-fwildclusterboot/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;When you suspect that the error terms in your regression model are correlated within clusters, and the number of clusters is small, trouble might be running at you. In such a situation, common cluster robust standard errors tend to be downward biased - they are too eager to reject the null hypothesis. Since &lt;a href=&#34;https://www.jstor.org/stable/40043157&#34;&gt;Cameron, Gelbach &amp;amp; Miller&lt;/a&gt; first suggested that the wild cluster bootstrap might be preferable to sandwich standard errors when the number of clusters is small, it has become common practice among empirical economists to check their cluster robust inferences against the wild cluster bootstrap.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:pressure&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;duerer_lion.jpg&#34; alt=&#34;Not a wild bootstrap, but a wild lion, by Albrecht Duerer&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Not a wild bootstrap, but a wild lion, by Albrecht Duerer
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;At some point, I found myself in a ‚Äúsmall number of clusters‚Äù situation. I was trying to estimate a treatment effect for a sample of a few thousand observations, which were grouped into around 20 clusters. So I started to search for R packages that implement the wild cluster bootstrap, and found two implementations on CRAN: &lt;code&gt;sandwich&lt;/code&gt; and &lt;code&gt;clusterSEs&lt;/code&gt;. I opted for the &lt;code&gt;sandwich&lt;/code&gt; package (because it‚Äôs actually a really great package) and fit my regression model via &lt;code&gt;lm()&lt;/code&gt;. Then I started to bootstrap with sandwich‚Äôs &lt;code&gt;vcovBS()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;So the bootstrap ran ‚Ä¶ and I waited. Eventually, I left my office to get some coffee with a colleague, returned to my desk ‚Ä¶ and the bootstrap still ran, and I waited even longer.&lt;/p&gt;
&lt;p&gt;But while the bootstrap was running, I scrolled the web and stumbled over the &lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/1536867X19830877?journalCode=stja&#34;&gt;‚ÄúFast &amp;amp; Wild‚Äù paper&lt;/a&gt; by Roodman et al (2019). The claimed performance in the paper seemed to good to be true: bootstrap inference with several thousands of iterations, in a fraction of a second? The paper presents a Stata implementation of the fast algorithm, &lt;a href=&#34;https://github.com/droodman/bottest&#34;&gt;boottest&lt;/a&gt;, and that was a good enough reason for me to start up a Stata session to try it out.&lt;/p&gt;
&lt;p&gt;And indeed, &lt;code&gt;boottest&lt;/code&gt; &lt;em&gt;is&lt;/em&gt; mind-blowingly fast: the bootstrap finished almost instantaneously. I was hooked: how was it possible that &lt;code&gt;boottest&lt;/code&gt; was &lt;em&gt;so damn fast&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;Luckily, the ‚ÄúFast &amp;amp; Wild‚Äù paper explains the algorithm powering &lt;code&gt;boottest&lt;/code&gt; in great detail. Out of curiosity, I started to implement it in R, and the &lt;code&gt;fwildclusterboot&lt;/code&gt; package is the result of this effort. Now, was it worth all the work? How much faster is the ‚Äúfast algorithm‚Äù implemented in &lt;code&gt;fwildclusterboot&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;To compare &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; performance to &lt;code&gt;sandwich&lt;/code&gt;, I simulate a data set with &lt;span class=&#34;math inline&#34;&gt;\(N = 10.000\)&lt;/span&gt; observations and &lt;span class=&#34;math inline&#34;&gt;\(N_G = 42\)&lt;/span&gt; distinct clusters (42 is the magic number of clusters for which the economics profession has decided that large N asymptotics fail, see Angrist &amp;amp; Pischke‚Äôs ‚ÄúMostly Harmless‚Äù, Chapter 8.2.3) and fit a regression model via &lt;code&gt;lm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fwildclusterboot)
library(sandwich)
library(lmtest)
library(bench)

# simulate data
seed &amp;lt;- 236723478
N &amp;lt;- 10000
data &amp;lt;- fwildclusterboot:::create_data(N = N,
                                         N_G1 = 42, icc1 = 0.1,
                                         N_G2 = 20, icc2 = 0.8,
                                         numb_fe1 = 10,
                                         numb_fe2 = 10,
                                         seed = seed,
                                         weights = 1:N)
lm_fit &amp;lt;- lm(proposition_vote ~ treatment + as.factor(Q1_immigration) + as.factor(Q2_defense), data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the first experiment, the bootstrap will run for &lt;span class=&#34;math inline&#34;&gt;\(B = 9999\)&lt;/span&gt; iterations. For the estimation via &lt;code&gt;vcovBS&lt;/code&gt;, we will use 4 cores.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 9999
# wild cluster bootstrap via sandwich::vcovBS

bench1 &amp;lt;- 
bench::mark(
  boot_slow = sandwich::vcovBS(lm_fit,
                                R = B,
                                cluster = ~ group_id1,
                                cores = 4), 
  iterations = 1
)
bench1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##   expression      min   median `itr/sec` mem_alloc `gc/sec`
##   &amp;lt;bch:expr&amp;gt; &amp;lt;bch:tm&amp;gt; &amp;lt;bch:tm&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;bch:byt&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 boot_slow     36.9s    36.9s    0.0271    12.7MB        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;vcovBS()&lt;/code&gt; finishes in around 37 seconds - that‚Äôs not too bad, isn‚Äôt it?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# wild cluster bootstrap via fwildclusterboot::boottest()
bench1f &amp;lt;- 
bench::mark(boot_fast =
                   fwildclusterboot::boottest(lm_fit,
                                              clustid = c(&amp;quot;group_id1&amp;quot;),
                                              B = B,
                                              param = &amp;quot;treatment&amp;quot;,
                                              seed = 3,
                                              nthreads = 1), 
            iterations = 25)
bench1f&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##   expression      min   median `itr/sec` mem_alloc `gc/sec`
##   &amp;lt;bch:expr&amp;gt; &amp;lt;bch:tm&amp;gt; &amp;lt;bch:tm&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;bch:byt&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 boot_fast    73.3ms   81.7ms      9.48    98.7MB     26.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While &lt;code&gt;sandwich::vcovBS()&lt;/code&gt; takes almost 36.9 seconds, &lt;code&gt;fwildclusterboot::boottest()&lt;/code&gt; runs in around one fifth of a second üöÄ. Yes, really: one fifth of a second! That‚Äôs a speed gain of a factor of 451! If you don‚Äôt have 4 cores available, performance differences get even more extreme (e.g.¬†if you only have one core, you have to multiply 37 with a number slightly smaller than 4).&lt;/p&gt;
&lt;p&gt;How do &lt;code&gt;vcovBS()&#39;s&lt;/code&gt; and &lt;code&gt;boottest()&#39;s&lt;/code&gt; results compare?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(boot_fast)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## boottest.lm(object = lm_fit, clustid = c(&amp;quot;group_id1&amp;quot;), param = &amp;quot;treatment&amp;quot;, 
##     B = B, seed = 3, nthreads = 1)
##  
##  Hypothesis: 1*treatment = 0
##  Observations: 10000
##  Bootstr. Iter: 9999
##  Bootstr. Type: rademacher
##  Clustering: 1-way
##  Confidence Sets: 95%
##  Number of Clusters: 42
## &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              term estimate statistic p.value conf.low conf.high
## 1 1*treatment = 0    0.002     0.516   0.605   -0.007     0.012&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmtest::coeftest(x = lm_fit, vcov = boot_slow)[2,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Estimate  Std. Error     t value    Pr(&amp;gt;|t|) 
## 0.002387792 0.004571759 0.522291836 0.601478745&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmtest::coefci(x = lm_fit, vcov = boot_slow)[2,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        2.5 %       97.5 % 
## -0.006573777  0.011349362&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Between the two implementations, the bootstrapped t-statistics, p-values and confidence intervals are almost identical. They are not exactly identical for two reasons: first due to sampling uncertainty in the bootstrap, and second because &lt;code&gt;vcovBS&lt;/code&gt; does not apply any small sample adjustments (at least I could not find anything related to small-sample adjustments in both documentation and source code).&lt;/p&gt;
&lt;p&gt;The speed gains of &lt;code&gt;fwildclusterboot&lt;/code&gt; scale well in the number of bootstrap iterations. For &lt;span class=&#34;math inline&#34;&gt;\(B = 99.999\)&lt;/span&gt; iterations, it finishes in around one second. For &lt;code&gt;vcovBS&lt;/code&gt;, you can expect a linear increase in run-time in the number of bootstrap iterations: a ten-fold increase in bootstrap iterations will increase run-time to around 360 seconds.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 99999

bench2f &amp;lt;- 
bench::mark(
  boot_fast =
    fwildclusterboot::boottest(lm_fit,
                             clustid = c(&amp;quot;group_id1&amp;quot;),
                             B = B,
                             param = &amp;quot;treatment&amp;quot;,
                             seed = 3,
                             nthreads = 1), 
  iterations = 10
)

bench2f&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##   expression      min   median `itr/sec` mem_alloc `gc/sec`
##   &amp;lt;bch:expr&amp;gt; &amp;lt;bch:tm&amp;gt; &amp;lt;bch:tm&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;bch:byt&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 boot_fast     476ms    571ms      1.72     727MB     11.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happens if we increase the sample size to &lt;span class=&#34;math inline&#34;&gt;\(N = 100.000\)&lt;/span&gt;?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N &amp;lt;- 100000
data &amp;lt;- fwildclusterboot:::create_data(N = N,
                                         N_G1 = 50, icc1 = 0.1,
                                         N_G2 = 20, icc2 = 0.8,
                                         numb_fe1 = 10,
                                         numb_fe2 = 10,
                                         seed = seed,
                                         weights = 1:N)
lm_fit &amp;lt;- lm(proposition_vote ~ treatment + as.factor(Q1_immigration) + as.factor(Q2_defense), data)
B &amp;lt;- 9999
# wild cluster bootstrap via sandwich::vcovBS
bench3 &amp;lt;- bench::mark(
  boot_slow = sandwich::vcovBS(lm_fit,
                                R = B,
                                cluster = ~ group_id1,
                                cores = 4), 
  iterations = 1)
bench3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##   expression      min   median `itr/sec` mem_alloc `gc/sec`
##   &amp;lt;bch:expr&amp;gt; &amp;lt;bch:tm&amp;gt; &amp;lt;bch:tm&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;bch:byt&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 boot_slow     8.32m    8.32m   0.00200    31.2MB        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More than 8 minutes pass before &lt;code&gt;vcovBS()&lt;/code&gt; finishes. How does &lt;code&gt;boottest()&lt;/code&gt; do in comparison?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# wild cluster bootstrap via fwildclusterboot::boottest()

bench3f &amp;lt;- 
bench::mark(
  boot_fast =
    fwildclusterboot::boottest(lm_fit,
                             clustid = c(&amp;quot;group_id1&amp;quot;),
                             B = B,
                             param = &amp;quot;treatment&amp;quot;,
                             seed = 3,
                             nthreads = 1), 
iterations = 5)
bench3f&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##   expression      min   median `itr/sec` mem_alloc `gc/sec`
##   &amp;lt;bch:expr&amp;gt; &amp;lt;bch:tm&amp;gt; &amp;lt;bch:tm&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;bch:byt&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 boot_fast     310ms    333ms      2.68     308MB     10.7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With &lt;code&gt;B = 9999&lt;/code&gt; iterations, &lt;code&gt;boottest()&lt;/code&gt; runs for around 0.33 seconds, while &lt;code&gt;vcovBS()&lt;/code&gt; only finishes after 499.36 seconds. &lt;code&gt;fwildclusterboot::boottest()&lt;/code&gt; is 1499 times faster than &lt;code&gt;sandwich::vcovBS&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;As a conclusion: if you face a ‚Äúsmall number of clusters‚Äù problem and want to reduce your daily ‚òï consumption, you should consider using &lt;a href=&#34;https://github.com/s3alfisc/fwildclusterboot&#34;&gt;fwildclusterboot&lt;/a&gt;, Stata‚Äôs &lt;a href=&#34;https://github.com/droodman/boottest&#34;&gt;boottest&lt;/a&gt;, or &lt;a href=&#34;https://github.com/droodman/WildBootTests.jl&#34;&gt;WildBootTests.jl&lt;/a&gt;, which is a novel Julia implementation of the ‚Äúfast algorithm‚Äù. If all of this seems like black magic to you and you want to learn more about the ‚Äúfast algorithm‚Äù, I cannot recommend the ‚ÄúFast &amp;amp; Wild‚Äù paper highly enough.&lt;/p&gt;
&lt;div id=&#34;literature&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Literature&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;‚ÄúFast &amp;amp; Wild‚Äù, Roodman et al.¬†(2019), The Stata Journal&lt;/li&gt;
&lt;li&gt;‚ÄúBootstrap-Based Improvements for Inference with Clustered Errors‚Äù, Cameron, Gelbach &amp;amp; Miller (2008), The Review of Economics and Statistics&lt;/li&gt;
&lt;li&gt;‚ÄúCluster-robust inference: A guide to empirical practice‚Äù (2020), MacKinnon, Oerregaard Nielsen &amp;amp; Webb, Working Paper&lt;/li&gt;
&lt;li&gt;‚ÄúMostly Harmless Econometrics‚Äù, Angrist &amp;amp; Pischke (2009), Princeton University Press&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>    </item>
    
  </channel>
</rss>
