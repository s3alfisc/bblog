<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>    <link>https://s3alfisc.github.io/blog/post/</link>
    <description>Recent blog posts by </description>    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 24 Jan 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://s3alfisc.github.io/blog/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Hosting a blogdown blog on github pages</title>
      <link>https://s3alfisc.github.io/blog/post/2022-01-24-hosting-a-blogdown-blog-on-github-pages/</link>
      <pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://s3alfisc.github.io/blog/post/2022-01-24-hosting-a-blogdown-blog-on-github-pages/</guid>
   <description>
&lt;script src=&#34;https://s3alfisc.github.io/blog/post/2022-01-24-hosting-a-blogdown-blog-on-github-pages/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I have recently moved from building this blog via the &lt;a href=&#34;https://github.com/rstudio/distill&#34;&gt;distill&lt;/a&gt; package to &lt;a href=&#34;https://github.com/rstudio/blogdown&#34;&gt;blogdown&lt;/a&gt;. The main reason for this is that, at the time of writing, distill does not support &lt;a href=&#34;https://github.com/rstudio/distill/issues/33&#34;&gt;full RSS feeds for multiple articles&lt;/a&gt;, which is a requirement for linking my blog to &lt;a href=&#34;r-bloggers.com&#34;&gt;R-bloggers&lt;/a&gt;. Deploying the &lt;code&gt;distill&lt;/code&gt; based blog via github pages was quite straightforward, but doing so for the &lt;code&gt;blogdown&lt;/code&gt; based blog proved to be slightly more cumbersome.&lt;/p&gt;
&lt;p&gt;While there are many good blog posts on deploying blogdown-blogs on github pages (e.g. &lt;a href=&#34;https://www.r-bloggers.com/2019/09/start-blogging-in-5-minutes-on-netlify-with-hugo-and-blogdown-september-2019-update/&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;https://www.caitlincasar.com/post/blogdown/&#34;&gt;here&lt;/a&gt;), I appear not to be the only one who ran into problems, as this &lt;a href=&#34;https://stackoverflow.com/questions/45362628/github-pages-site-not-detecting-index-html&#34;&gt;stackoverflow thread&lt;/a&gt; with 100+ upvotes shows.&lt;/p&gt;
&lt;p&gt;In my case, I needed to do two things for successful deployment on github pages:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Add a &lt;code&gt;.nojekyll&lt;/code&gt; file to the main directory of the blog, e.g. by running &lt;code&gt;file.create(&#34;.nojekyll&#34;)&lt;/code&gt; in the r console&lt;/li&gt;
&lt;li&gt;Add a &lt;code&gt;publishDir: docs&lt;/code&gt; statement below &lt;code&gt;baseurl&lt;/code&gt; in the &lt;code&gt;config.yaml&lt;/code&gt; file. Rebuilding via &lt;code&gt;blogdown::build_site()&lt;/code&gt; then creates a docs folder and populates it with html. On github pages, I then needed to make sure that the blog is build based on this docs folder (see the image below). After that, I simply had to commit, push, and github actions would finally deploy the blog without any error messages!&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:pressure&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;github_pages.PNG&#34; alt=&#34;Build the github pages site from the &#39;docs&#39; folder&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Build the github pages site from the ‘docs’ folder
&lt;/p&gt;
&lt;/div&gt;
</description>    </item>
    
    <item>
      <title>Testing an R package against a Julia package on github actions</title>
      <link>https://s3alfisc.github.io/blog/post/testing-an-r-package-against-a-julia-package-via-github-actions/</link>
      <pubDate>Sat, 18 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://s3alfisc.github.io/blog/post/testing-an-r-package-against-a-julia-package-via-github-actions/</guid>
   <description>
&lt;script src=&#34;https://s3alfisc.github.io/blog/post/testing-an-r-package-against-a-julia-package-via-github-actions/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Something I have learned the hard way: every single line of code that I write should be accompanied by a unit test. Unit tests are not only useful for me as a developer to spot bugs, but a thorough sequence of unit tests should also increase others’ confidence in the quality of my code.&lt;/p&gt;
&lt;p&gt;For statistical methods and algorithms, one great way to test if everything is working as intended is to test one’s own implementation of an algorithm against someone else’s.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:pressure&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;Caravaggio_Sacrifice_of_Isaac.jpg&#34; alt=&#34;Abraham is *severely* tested. (Sacrifice of Isaac, Caravaggio, ca. 1598. Barbara Piasecka-Johnson Collection.)&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Abraham is &lt;em&gt;severely&lt;/em&gt; tested. (Sacrifice of Isaac, Caravaggio, ca. 1598. Barbara Piasecka-Johnson Collection.)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This idea is expressed in &lt;a href=&#34;https://stats-devguide.ropensci.org/standards.html&#34;&gt;ropensci’s statistical software standards&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;G5.4 Correctness tests to test that statistical algorithms produce expected results to some fixed test data sets (potentially through comparisons using binding frameworks such as RStata).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;G5.4b For new implementations of existing methods, correctness tests should include tests against previous implementations. Such testing may explicitly call those implementations in testing, preferably from fixed-versions of other software, or use stored outputs from those where that is not possible.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Preferably, these unit tests should be automized via continuous integration tools such as github actions. CI is super useful - a CI workflow might e.g. trigger to run all tests at every commit, so I &lt;em&gt;definitely&lt;/em&gt; won’t forget to run them. Second, CI integration via github actions communicates to my package’s user that I have actually and successfully run all unit tests on the latest development version. Last, all tests are run “in the cloud”, so I am not blocking my own computer for (potentially) multiple minutes.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As expressed in ropensci’s guideline G5.4b, in scientific computing, the same algorithm might only be implemented just once, or, in the lucky case that another implementation exists, it might be written in another language.&lt;/p&gt;
&lt;p&gt;This applies to the “fast” cluster bootstrap algorithm implemented in &lt;a href=&#34;https://github.com/s3alfisc/fwildclusterboot&#34;&gt;fwildclusterboot&lt;/a&gt;. To my knowledge, there exists an R implementation (&lt;code&gt;fwildclusterboot&lt;/code&gt;), the original Stata version (&lt;a href=&#34;https://github.com/droodman/boottest&#34;&gt;boottest&lt;/a&gt;), and recently, David Roodman has written a Julia package, &lt;a href=&#34;https://github.com/droodman/WildBootTests.jl&#34;&gt;WildBootTests.jl&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Up until now, &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; test sequence has been divided in two separate parts. Part one’s job was to test if the methods for different regression packages in R producec consistent results (“internal consistency”). These tests could easily be run on CRAN and github actions, as they only required &lt;code&gt;fwildclusterboot&lt;/code&gt; and a working R distribution.&lt;/p&gt;
&lt;p&gt;Part two of the test sequence tested &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; “external validity” by running the bootstrap in both R and Stata (via &lt;a href=&#34;https://github.com/droodman/boottest&#34;&gt;boottest&lt;/a&gt; &amp;amp; the RStata package). Before a CRAN submission, I would run all these tests on my local machine. Of course, the unit tests involving Stata could not be run on CRAN, and I did not think (and am now uncertain) that these tests could be automatized via github actions.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As it happend, my Stata student license expired the other week, and when I checked StataCorps website for renewal fees, Stata - for non-academics - had a &lt;a href=&#34;https://dpc-onlineshop.de/epages/1188aeca-9275-49f6-be7a-ac117311e6ae.sf/de_DE/?ObjectPath=/Shops/1188aeca-9275-49f6-be7a-ac117311e6ae/Products/5170113&#34;&gt;840 € price tag for a one-year license&lt;/a&gt;. To me, this seemed to be a hefty price tag for running a few unit tests for &lt;code&gt;fwildclusterboot&lt;/code&gt;, and was a good incentive to update my unit tests.&lt;/p&gt;
&lt;p&gt;Luckily, &lt;code&gt;WildBootTests.jl&lt;/code&gt; is now around, and as Julia is open source, I was fairly certain that I could transfer my “external” tests from Stata to Julia and deploy all tests on github actions.&lt;/p&gt;
&lt;div id=&#34;how-to-set-up-a-workflow-that-tests-an-r-package-against-a-julia-package&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to set up a workflow that tests an R package against a Julia package&lt;/h3&gt;
&lt;p&gt;So, how does a workflow to test an R package against a Julia package looks like?&lt;/p&gt;
&lt;p&gt;First of all, I have used the skeleton of &lt;code&gt;fwildclusterboot&lt;/code&gt; to produce a wrapper around &lt;code&gt;WildBootTests.jl&lt;/code&gt;, &lt;a href=&#34;https://github.com/s3alfisc/wildboottestjlr&#34;&gt;wildboottestjlr&lt;/a&gt;. All data pre-processing is handled by functions copied from &lt;code&gt;fwildclusterboot&lt;/code&gt;. For running the bootstrap, R sends all required objects to Julia (via the excellent &lt;code&gt;JuliaConnectoR&lt;/code&gt; package, on which I will blog in the future). In a second step, I have added &lt;code&gt;wildboottestjlr&lt;/code&gt; to &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; dependencies and have initiated a github actions workflow via &lt;code&gt;usethis::use_github_actions()&lt;/code&gt;. (Note that it is in fact not neccessariy to write an entire R wrapper-package for the Julia algorithm you want to test. &lt;code&gt;wildboottestjlr&lt;/code&gt; exists because &lt;code&gt;WildBootTests.jl&lt;/code&gt; is actually a magnitude faster than &lt;code&gt;fwildclusterboot&lt;/code&gt; and because it has many additional feature, as e.g. a wild cluster bootstrap for instrumental variables.)&lt;/p&gt;
&lt;p&gt;The created workflow yaml looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For help debugging build failures open an issue on the RStudio community with the &amp;#39;github-actions&amp;#39; tag.
# https://community.rstudio.com/new-topic?category=Package%20development&amp;amp;tags=github-actions
on:
  push:
    branches:
      - main
      - master
  pull_request:
    branches:
      - main
      - master

name: R-CMD-check

jobs:
  R-CMD-check:
    runs-on: ${{ matrix.config.os }}

    name: ${{ matrix.config.os }} (${{ matrix.config.r }})

    strategy:
      fail-fast: false
      matrix:
        config:
          - {os: windows-latest, r: &amp;#39;release&amp;#39;}
          - {os: macOS-latest, r: &amp;#39;release&amp;#39;}
          - {os: ubuntu-20.04, r: &amp;#39;release&amp;#39;, rspm: &amp;quot;https://packagemanager.rstudio.com/cran/__linux__/focal/latest&amp;quot;}

    env:
      R_REMOTES_NO_ERRORS_FROM_WARNINGS: true
      RSPM: ${{ matrix.config.rspm }}
      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}

    steps:
      - uses: actions/checkout@v2

      - uses: r-lib/actions/setup-r@v2
        with:
          r-version: ${{ matrix.config.r }}

      - uses: r-lib/actions/setup-pandoc@v2

      - name: Query dependencies
        run: |
          install.packages(&amp;#39;remotes&amp;#39;)
          saveRDS(remotes::dev_package_deps(dependencies = TRUE), &amp;quot;.github/depends.Rds&amp;quot;, version = 2)
          writeLines(sprintf(&amp;quot;R-%i.%i&amp;quot;, getRversion()$major, getRversion()$minor), &amp;quot;.github/R-version&amp;quot;)
        shell: Rscript {0}

      - name: Cache R packages
        if: runner.os != &amp;#39;Windows&amp;#39;
        uses: actions/cache@v2
        with:
          path: ${{ env.R_LIBS_USER }}
          key: ${{ runner.os }}-${{ hashFiles(&amp;#39;.github/R-version&amp;#39;) }}-1-${{ hashFiles(&amp;#39;.github/depends.Rds&amp;#39;) }}
          restore-keys: ${{ runner.os }}-${{ hashFiles(&amp;#39;.github/R-version&amp;#39;) }}-1-

      - name: Install system dependencies
        if: runner.os == &amp;#39;Linux&amp;#39;
        run: |
          while read -r cmd
          do
            eval sudo $cmd
          done &amp;lt; &amp;lt;(Rscript -e &amp;#39;writeLines(remotes::system_requirements(&amp;quot;ubuntu&amp;quot;, &amp;quot;20.04&amp;quot;))&amp;#39;)
      - name: Install dependencies
        run: |
          remotes::install_deps(dependencies = TRUE)
          remotes::install_cran(&amp;quot;rcmdcheck&amp;quot;)
        shell: Rscript {0}
        
      # install julia
      - uses: julia-actions/setup-julia@v1
      # add julia to renviron
      - name: Create and populate .Renviron file
        run: echo JULIA_BINDIR= &amp;quot;${{ env.juliaLocation }}&amp;quot; &amp;gt;&amp;gt; ~/.Renviron
        shell: bash

      # install WildBootTests.jl
      - name: install WildBootTests.jl 
        run: julia -e &amp;#39;using Pkg; Pkg.add(&amp;quot;WildBootTests&amp;quot;)&amp;#39;
        # use shell bash to ensure consistent behavior across OS
        shell: bash

      - name: Check
        env:
          _R_CHECK_CRAN_INCOMING_REMOTE_: false
        run: rcmdcheck::rcmdcheck(args = c(&amp;quot;--no-manual&amp;quot;, &amp;quot;--as-cran&amp;quot;), error_on = &amp;quot;warning&amp;quot;, check_dir = &amp;quot;check&amp;quot;)
        shell: Rscript {0}

      - name: Upload check results
        if: failure()
        uses: actions/upload-artifact@main
        with:
          name: ${{ runner.os }}-r${{ matrix.config.r }}-results
          path: check&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a nutshell, this workflow installs R and &lt;code&gt;fwildclusterboot&lt;/code&gt; in a clean ubuntu environment, conducts a cmd-check and initiates all of &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; unit tests.&lt;/p&gt;
&lt;p&gt;To enable testing against Julia and &lt;code&gt;WildBootTests.jl&lt;/code&gt;, it further adds three ‘steps’ to the workflow: the workflow has to install &lt;code&gt;Julia&lt;/code&gt;, &lt;code&gt;WildBootTests.jl&lt;/code&gt; (and all its Julia dependencies) and link R and Julia so that &lt;code&gt;JuliaConnectoR&lt;/code&gt; could communicate between the two languages.&lt;/p&gt;
&lt;p&gt;This is easily be achieved by adding the lines below to the yaml file create by &lt;code&gt;usethis::use_github_action()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install julia
- uses: julia-actions/setup-julia@v1
# add julia to renviron
- name: Create and populate .Renviron file
  run: echo JULIA_BINDIR= &amp;quot;${{ env.juliaLocation }}&amp;quot; &amp;gt;&amp;gt; ~/.Renviron
  shell: bash

# install WildBootTests.jl
  - name: install WildBootTests.jl 
    run: julia -e &amp;#39;using Pkg; Pkg.add(&amp;quot;WildBootTests&amp;quot;)&amp;#39;
  # use shell bash to ensure consistent behavior across OS
    shell: bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Placing these few lines in the workflow yaml before the cmd check is triggered completes the workflow. It now downloads R and Julia, installs &lt;code&gt;fwildclusterboot&lt;/code&gt;, &lt;code&gt;wildboottestjlr&lt;/code&gt; and &lt;code&gt;WildBootTests.jl&lt;/code&gt;, links R and Julia, and runs all of &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; unit tests. The remaining step is to push to github, and then all unit tests are running!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The unit tests for fwildclusterboot currently run for about 30 minutes.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I have not implemented test that compare &lt;code&gt;fwildclusterboot&lt;/code&gt; against other R implementations of the wild bootstrap, mainly due to performance reasons. As resampling methods are usually time intensive and fwildclusterboot is &lt;a href=&#34;https://s3alfisc.github.io/post/1000x-faster-wild-cluster-bootstrap-inference-in-r-with-fwildclusterboot/&#34;&gt;much faster&lt;/a&gt; than other R implementations, running unit tests against other R implementations would simply take a &lt;em&gt;very&lt;/em&gt; long time.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>    </item>
    
    <item>
      <title>1000x faster Wild Cluster Bootstrap Inference in R with fwildclusterboot</title>
      <link>https://s3alfisc.github.io/blog/post/1000x-faster-wild-cluster-bootstrap-inference-in-r-with-fwildclusterboot/</link>
      <pubDate>Sun, 12 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://s3alfisc.github.io/blog/post/1000x-faster-wild-cluster-bootstrap-inference-in-r-with-fwildclusterboot/</guid>
   <description>
&lt;script src=&#34;https://s3alfisc.github.io/blog/post/1000x-faster-wild-cluster-bootstrap-inference-in-r-with-fwildclusterboot/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;When you suspect that the error terms in your regression model are correlated within clusters, and the number of clusters is small, trouble might be running at you. In such a situation, common cluster robust standard errors tend to be downward biased - they are too eager to reject the null hypothesis. Since &lt;a href=&#34;https://www.jstor.org/stable/40043157&#34;&gt;Cameron, Gelbach &amp;amp; Miller&lt;/a&gt; first suggested that the wild cluster bootstrap might be preferable to sandwich standard errors when the number of clusters is small, it has become common practice among empirical economists to check their cluster robust inferences against the wild cluster bootstrap.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:pressure&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;duerer_lion.jpg&#34; alt=&#34;Not a wild bootstrap, but a wild lion, by Albrecht Duerer&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Not a wild bootstrap, but a wild lion, by Albrecht Duerer
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;At some point, I found myself in a “small number of clusters” situation. I was trying to estimate a treatment effect for a sample of a few thousand observations, which were grouped into around 20 clusters. So I started to search for R packages that implement the wild cluster bootstrap, and found two implementations on CRAN: &lt;code&gt;sandwich&lt;/code&gt; and &lt;code&gt;clusterSEs&lt;/code&gt;. I opted for the &lt;code&gt;sandwich&lt;/code&gt; package (because it’s actually a really great package) and fit my regression model via &lt;code&gt;lm()&lt;/code&gt;. Then I started to bootstrap with sandwich’s &lt;code&gt;vcovBS()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;So the bootstrap ran … and I waited. Eventually, I left my office to get some coffee with a colleague, returned to my desk … and the bootstrap still ran, and I waited even longer.&lt;/p&gt;
&lt;p&gt;But while the bootstrap was running, I scrolled the web and stumbled over the &lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/1536867X19830877?journalCode=stja&#34;&gt;“Fast &amp;amp; Wild” paper&lt;/a&gt; by Roodman et al (2019). The claimed performance in the paper seemed to good to be true: bootstrap inference with several thousands of iterations, in a fraction of a second? The paper presents a Stata implementation of the fast algorithm, &lt;a href=&#34;https://github.com/droodman/bottest&#34;&gt;boottest&lt;/a&gt;, and that was a good enough reason for me to start up a Stata session to try it out.&lt;/p&gt;
&lt;p&gt;And indeed, &lt;code&gt;boottest&lt;/code&gt; &lt;em&gt;is&lt;/em&gt; mind-blowingly fast: the bootstrap finished almost instantaneously. I was hooked: how was it possible that &lt;code&gt;boottest&lt;/code&gt; was &lt;em&gt;so damn fast&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;Luckily, the “Fast &amp;amp; Wild” paper explains the algorithm powering &lt;code&gt;boottest&lt;/code&gt; in great detail. Out of curiosity, I started to implement it in R, and the &lt;code&gt;fwildclusterboot&lt;/code&gt; package is the result of this effort. Now, was it worth all the work? How much faster is the “fast algorithm” implemented in &lt;code&gt;fwildclusterboot&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;To compare &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; performance to &lt;code&gt;sandwich&lt;/code&gt;, I simulate a data set with &lt;span class=&#34;math inline&#34;&gt;\(N = 10.000\)&lt;/span&gt; observations and &lt;span class=&#34;math inline&#34;&gt;\(N_G = 42\)&lt;/span&gt; distinct clusters (42 is the magic number of clusters for which the economics profession has decided that large N asymptotics fail, see Angrist &amp;amp; Pischke’s “Mostly Harmless”, Chapter 8.2.3) and fit a regression model via &lt;code&gt;lm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fwildclusterboot)
library(sandwich)
library(lmtest)
library(bench)

# simulate data
seed &amp;lt;- 236723478
N &amp;lt;- 10000
data &amp;lt;- fwildclusterboot:::create_data(N = N,
                                         N_G1 = 42, icc1 = 0.1,
                                         N_G2 = 20, icc2 = 0.8,
                                         numb_fe1 = 10,
                                         numb_fe2 = 10,
                                         seed = seed,
                                         weights = 1:N)
lm_fit &amp;lt;- lm(proposition_vote ~ treatment + as.factor(Q1_immigration) + as.factor(Q2_defense), data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the first experiment, the bootstrap will run for &lt;span class=&#34;math inline&#34;&gt;\(B = 9999\)&lt;/span&gt; iterations. For the estimation via &lt;code&gt;vcovBS&lt;/code&gt;, we will use 4 cores.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 9999
# wild cluster bootstrap via sandwich::vcovBS

bench1 &amp;lt;- 
bench::mark(
  boot_slow = sandwich::vcovBS(lm_fit,
                                R = B,
                                cluster = ~ group_id1,
                                cores = 4), 
  iterations = 1
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;vcovBS()&lt;/code&gt; finishes in around 30 seconds - that’s not too bad, isn’t it?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# wild cluster bootstrap via fwildclusterboot::boottest()
bench1f &amp;lt;- 
bench::mark(boot_fast =
                   fwildclusterboot::boottest(lm_fit,
                                              clustid = c(&amp;quot;group_id1&amp;quot;),
                                              B = B,
                                              param = &amp;quot;treatment&amp;quot;,
                                              seed = 3,
                                              nthreads = 1), 
            iterations = 25)
bench1f&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##   expression      min   median `itr/sec` mem_alloc `gc/sec`
##   &amp;lt;bch:expr&amp;gt; &amp;lt;bch:tm&amp;gt; &amp;lt;bch:tm&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;bch:byt&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 boot_fast    75.1ms   84.7ms      8.72      99MB     29.7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While &lt;code&gt;sandwich::vcovBS()&lt;/code&gt; takes almost 30.42 seconds, &lt;code&gt;fwildclusterboot::boottest()&lt;/code&gt; runs in around one fifth of a second 🚀. Yes, really: one fifth of a second! That’s a speed gain of a factor of 359! If you don’t have 4 cores available, performance differences get even more extreme (e.g. if you only have one core, you have to multiply 30 with a number slightly smaller than 4).&lt;/p&gt;
&lt;p&gt;How do &lt;code&gt;vcovBS()&#39;s&lt;/code&gt; and &lt;code&gt;boottest()&#39;s&lt;/code&gt; results compare?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(boot_fast)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## boottest.lm(object = lm_fit, clustid = c(&amp;quot;group_id1&amp;quot;), param = &amp;quot;treatment&amp;quot;, 
##     B = B, seed = 3, nthreads = 1)
##  
##  Hypothesis: 1*treatment = 0
##  Observations: 10000
##  Bootstr. Iter: 9999
##  Bootstr. Type: rademacher
##  Clustering: 1-way
##  Confidence Sets: 95%
##  Number of Clusters: 42
## &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              term estimate statistic p.value conf.low conf.high
## 1 1*treatment = 0    0.002     0.516   0.605   -0.007     0.012&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmtest::coeftest(x = lm_fit, vcov = boot_slow)[2,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Estimate  Std. Error     t value    Pr(&amp;gt;|t|) 
## 0.002387792 0.004592932 0.519884080 0.603155903&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmtest::coefci(x = lm_fit, vcov = boot_slow)[2,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        2.5 %       97.5 % 
## -0.006615281  0.011390866&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Between the two implementations, the bootstrapped t-statistics, p-values and confidence intervals are almost identical. They are not exactly identical for two reasons: first due to sampling uncertainty in the bootstrap, and second because &lt;code&gt;vcovBS&lt;/code&gt; does not apply any small sample adjustments (at least I could not find anything related to small-sample adjustments in both documentation and source code).&lt;/p&gt;
&lt;p&gt;The speed gains of &lt;code&gt;fwildclusterboot&lt;/code&gt; scale well in the number of bootstrap iterations. For &lt;span class=&#34;math inline&#34;&gt;\(B = 99.999\)&lt;/span&gt; iterations, it finishes in around one second. For &lt;code&gt;vcovBS&lt;/code&gt;, you can expect a linear increase in run-time in the number of bootstrap iterations: a ten-fold increase in bootstrap iterations will increase run-time to around 360 seconds.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 99999

bench2f &amp;lt;- 
bench::mark(
  boot_fast =
    fwildclusterboot::boottest(lm_fit,
                             clustid = c(&amp;quot;group_id1&amp;quot;),
                             B = B,
                             param = &amp;quot;treatment&amp;quot;,
                             seed = 3,
                             nthreads = 1), 
  iterations = 10
)

bench2f&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##   expression      min   median `itr/sec` mem_alloc `gc/sec`
##   &amp;lt;bch:expr&amp;gt; &amp;lt;bch:tm&amp;gt; &amp;lt;bch:tm&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;bch:byt&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 boot_fast     541ms    553ms      1.77     727MB     15.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happens if we increase the sample size to &lt;span class=&#34;math inline&#34;&gt;\(N = 100.000\)&lt;/span&gt;?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N &amp;lt;- 100000
data &amp;lt;- fwildclusterboot:::create_data(N = N,
                                         N_G1 = 50, icc1 = 0.1,
                                         N_G2 = 20, icc2 = 0.8,
                                         numb_fe1 = 10,
                                         numb_fe2 = 10,
                                         seed = seed,
                                         weights = 1:N)
lm_fit &amp;lt;- lm(proposition_vote ~ treatment + as.factor(Q1_immigration) + as.factor(Q2_defense), data)
B &amp;lt;- 9999
# wild cluster bootstrap via sandwich::vcovBS
bench3 &amp;lt;- bench::mark(
  boot_slow = sandwich::vcovBS(lm_fit,
                                R = B,
                                cluster = ~ group_id1,
                                cores = 4), 
  iterations = 1)
bench3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##   expression      min   median `itr/sec` mem_alloc `gc/sec`
##   &amp;lt;bch:expr&amp;gt; &amp;lt;bch:tm&amp;gt; &amp;lt;bch:tm&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;bch:byt&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 boot_slow     7.87m    7.87m   0.00212    31.2MB        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More than 7 minutes pass before &lt;code&gt;vcovBS()&lt;/code&gt; finishes. How does &lt;code&gt;boottest()&lt;/code&gt; do in comparison?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# wild cluster bootstrap via fwildclusterboot::boottest()

bench3f &amp;lt;- 
bench::mark(
  boot_fast =
    fwildclusterboot::boottest(lm_fit,
                             clustid = c(&amp;quot;group_id1&amp;quot;),
                             B = B,
                             param = &amp;quot;treatment&amp;quot;,
                             seed = 3,
                             nthreads = 1), 
iterations = 5)
bench3f&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##   expression      min   median `itr/sec` mem_alloc `gc/sec`
##   &amp;lt;bch:expr&amp;gt; &amp;lt;bch:tm&amp;gt; &amp;lt;bch:tm&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;bch:byt&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 boot_fast     331ms    352ms      2.57     308MB     9.78&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With &lt;code&gt;B = 9999&lt;/code&gt; iterations, &lt;code&gt;boottest()&lt;/code&gt; runs for around 0.35 seconds, while &lt;code&gt;vcovBS()&lt;/code&gt; only finishes after 472.16 seconds. &lt;code&gt;fwildclusterboot::boottest()&lt;/code&gt; is 1340 times faster than &lt;code&gt;sandwich::vcovBS&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;As a conclusion: if you face a “small number of clusters” problem and want to reduce your daily ☕ consumption, you should consider using &lt;a href=&#34;https://github.com/s3alfisc/fwildclusterboot&#34;&gt;fwildclusterboot&lt;/a&gt;, Stata’s &lt;a href=&#34;https://github.com/droodman/boottest&#34;&gt;boottest&lt;/a&gt;, or &lt;a href=&#34;https://github.com/droodman/WildBootTests.jl&#34;&gt;WildBootTests.jl&lt;/a&gt;, which is a novel Julia implementation of the “fast algorithm”. If all of this seems like black magic to you and you want to learn more about the “fast algorithm”, I cannot recommend the “Fast &amp;amp; Wild” paper highly enough.&lt;/p&gt;
&lt;div id=&#34;literature&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Literature&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;“Fast &amp;amp; Wild”, Roodman et al. (2019), The Stata Journal&lt;/li&gt;
&lt;li&gt;“Bootstrap-Based Improvements for Inference with Clustered Errors”, Cameron, Gelbach &amp;amp; Miller (2008), The Review of Economics and Statistics&lt;/li&gt;
&lt;li&gt;“Cluster-robust inference: A guide to empirical practice” (2020), MacKinnon, Oerregaard Nielsen &amp;amp; Webb, Working Paper&lt;/li&gt;
&lt;li&gt;“Mostly Harmless Econometrics”, Angrist &amp;amp; Pischke (2009), Princeton University Press&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>    </item>
    
  </channel>
</rss>
