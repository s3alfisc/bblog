<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>    <link>https://s3alfisc.github.io/blog/post/</link>
    <description>Recent blog posts by </description>    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 29 Jan 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://s3alfisc.github.io/blog/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A üê¥ race: The Wild Cluster Bootstrap vs Satterthwaite-corrected Sandwich Estimators when the number of Clusters is small</title>
      <link>https://s3alfisc.github.io/blog/post/2022-01-29-cluster-robust-inference-when-the-number-of-clusters-is-small-a-horse-race/</link>
      <pubDate>Sat, 29 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://s3alfisc.github.io/blog/post/2022-01-29-cluster-robust-inference-when-the-number-of-clusters-is-small-a-horse-race/</guid>
   <description>
&lt;script src=&#34;https://s3alfisc.github.io/blog/post/2022-01-29-cluster-robust-inference-when-the-number-of-clusters-is-small-a-horse-race/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;A couple of months ago, Gordon Burtch shared an excellent Twitter thread on the merits of wild cluster bootstrap inference when the regression error terms are clustered into a small group of clusters:&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;How biased are clustered SEs with &amp;#39;few&amp;#39; clusters? A simulation illustrating this. DGP is y~x, 50 clusters, x is normal, true beta is 0.5. Plot of 1000 sims, beta estimate +95% CIs for each. Red = we did not cover true beta. Std SEs no good, clustered SEs yield ~95% coverage (1/6) &lt;a href=&#34;https://t.co/z3eZdy1wb1&#34;&gt;pic.twitter.com/z3eZdy1wb1&lt;/a&gt;&lt;/p&gt;&amp;mdash; Gord Burtch (@gburtch) &lt;a href=&#34;https://twitter.com/gburtch/status/1378520203689082886?ref_src=twsrc%5Etfw&#34;&gt;April 4, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;!-- &lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;How biased are clustered SEs with &amp;#39;few&amp;#39; clusters? A simulation illustrating this. DGP is y~x, 50 clusters, x is normal, true beta is 0.5. Plot of 1000 sims, beta estimate +95% CIs for each. Red = we did not cover true beta. Std SEs no good, clustered SEs yield ~95% coverage (1/6) &lt;a href=&#34;https://t.co/z3eZdy1wb1&#34;&gt;pic.twitter.com/z3eZdy1wb1&lt;/a&gt;&lt;/p&gt;&amp;mdash; Gord Burtch (@gburtch) &lt;a href=&#34;https://twitter.com/gburtch/status/1378520203689082886?ref_src=twsrc%5Etfw&#34;&gt;April 4, 2021&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;  --&gt;
&lt;p&gt;In his simulation study, Gordon focused on the wild cluster bootstrap vs regular sandwich cluster robust inference (CRVE). As I am quite invested in the wild cluster bootstrap, I was happy to see that it appeared to outperform ‚Äòclassical‚Äô robust standard errors: in his simulations, the coverage rate of wild clustered bootstrapped confidence intervals is already close to the desired coverage rate of 95% even for a small number of clusters (e.g.¬†5-10).&lt;/p&gt;
&lt;!-- Have you ever heard of Satterthwaite degree-of-freedom bias-corrections for cluster robust standard error estimation? --&gt;
&lt;!-- Well, I myself have never seen them used in any applied econometrics paper \footnote{As it happens, Angrist \&amp; Lavy ... }. But while writing this blog post, I realized that both the survey paper by [Cameron &amp; Miller](http://cameron.econ.ucdavis.edu/research/Cameron_Miller_JHR_2015_February.pdf) and Mostly Harmless Econometrics both discuss ... --&gt;
&lt;p&gt;Nevertheless, there is a statistical literature that argues that it is fine to use cluster robust sandwich estimators to compute standard errors for a small number of clusters as long as one applies an appropriate &lt;strong&gt;small sample correction&lt;/strong&gt; via &lt;strong&gt;Satterthwaite&lt;/strong&gt; or &lt;strong&gt;saddlepoint corrections&lt;/strong&gt; (Imbens &amp;amp; Kolesar, Bell, Tipton &amp;amp; Pustejovksy).
All these methods are implemented in R via the &lt;code&gt;clubSandwich&lt;/code&gt; package and in Stata in the &lt;a href=&#34;https://github.com/jepusto/clubSandwich-Stata&#34;&gt;clubSandwich-Stata&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;Of course I was curious to see how the Satterthwaite-corrected SEs would perform in comparison to the would cluster bootstrap, so I decided to run some simulations.&lt;/p&gt;
&lt;p&gt;Luckily for me, Gordon published all of his code &lt;a href=&#34;https://github.com/gburtch/simulating_cluster_SEs&#34;&gt;on github&lt;/a&gt;, so it was easy for me to slightly tweak it and add simulations for Satterthwaite corrections. Open software is really awesome!&lt;/p&gt;
&lt;p&gt;I have collected my minor updates of Gordon‚Äôs code in an R package, which is available on &lt;a href=&#34;https://github.com/s3alfisc/clusteredErrorsSims&#34;&gt;github&lt;/a&gt;. To reproduce all analyses below, you simply have to install the package by running&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install.packages(&amp;quot;devtools&amp;quot;)
devtools::install_github(&amp;quot;s3alfisc/clusteredErrorsSims&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But before we dive into the simulations, I will start with revising some theory on the consistency of CRVE that will motivate the design of the simulations.&lt;/p&gt;
&lt;div id=&#34;when-are-clustered-standard-errors-biased&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;When are clustered standard errors biased?&lt;/h2&gt;
&lt;p&gt;In general, cluster robust variance estimators will be biased if one of the three conditions below holds:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If there are only very few clusters.&lt;/li&gt;
&lt;li&gt;If the cluster sizes are wildly different.&lt;/li&gt;
&lt;li&gt;If the intra-cluster correlations varies across clusters.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the following simulations, I will focus on cases 1-2 and conduct three simulation studies. The &lt;strong&gt;first&lt;/strong&gt; simulation closely follows Gordon‚Äôs work and investigates the performances of different inference methods for a &lt;strong&gt;small number of clusters G&lt;/strong&gt;, but includes simulations for Satterthwaite corrected CRVE estimates via the &lt;code&gt;clubSandwich&lt;/code&gt; package. The &lt;strong&gt;second&lt;/strong&gt; set of simulations investigates the performance for clustered erros with &lt;span class=&#34;math inline&#34;&gt;\(G \in \{50, 100\}\)&lt;/span&gt; clusters, but &lt;strong&gt;wildly different&lt;/strong&gt; cluster sizes. Last, I take a look at a special case that has received considerable attention: how do wild cluster bootstrap and Satterthwaite corrected SEs perform when only few clusters are treated (as often happens with Difference-in-Differences identification strategies)? Simulations 2 and 3 are heavily influenced by work by &lt;a href=&#34;https://ageconsearch.umn.edu/record/274639/files/qed_wp_1314.pdf&#34;&gt;MacKinnon &amp;amp; Webb&lt;/a&gt; on the performance of the wild cluster bootstrap under ‚Äúwildly different‚Äù cluster sizes.&lt;/p&gt;
&lt;p&gt;The data generating process for all simulations is a simple linear regression model for &lt;span class=&#34;math inline&#34;&gt;\(g = 1, ..., G\)&lt;/span&gt; clusters:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  y_{ig} = \beta_0 + \beta_1 X_{ig} + \epsilon_{ig}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(E(\epsilon_{ig}|X_{ig}) = 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 = 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 = 0.5\)&lt;/span&gt; and the errors &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ig}\)&lt;/span&gt; are simulated to be correlated within &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; clusters with intra-cluster correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. All errors are uncorrelated across clusters.&lt;/p&gt;
&lt;p&gt;So the stage is set for a üê¥ race! My champion, of course, is the wild cluster bootstrap, but let‚Äôs see how the bias-corrected standard errors perform in comparison!&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:pressure&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;Le_jockey.jpg&#34; alt=&#34;Toulose-Lautrec, Le Jockey, 1899&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Toulose-Lautrec, Le Jockey, 1899
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation-1-small-number-of-clusters-g-balanced-cluster-sizes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Simulation 1: Small number of clusters G &amp;amp; balanced cluster sizes&lt;/h4&gt;
&lt;p&gt;To initiate the horse race, you simply have to run the &lt;code&gt;sim_balanced_clusters()&lt;/code&gt; function, though I want to note that on my laptop, this takes around 2h while using multiple cores. By default, the wild cluster bootstrap will run with &lt;span class=&#34;math inline&#34;&gt;\(B = 9999\)&lt;/span&gt; bootstrap iterations throughout all simulations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(clusteredErrorsSims)
set.seed(1234)
sim_balanced_clusters(n = 1000, n_sims = 1000, rho = 0.7, workers = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the results for the first simulation:&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:Result1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;CI%20Coverage%20for%20Several%20Cluster%20Robust%20Inference%20Methods.png&#34; alt=&#34;Simulation results. N = 1000, rho = 0.7, balanced cluster sizes&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Simulation results. N = 1000, rho = 0.7, balanced cluster sizes
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;There are three takeaways from figure 2:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As expected, inference with non-robust standard errors is severely biased.&lt;/li&gt;
&lt;li&gt;Inference via cluster robust estimators tends to under-reject for less than 50 clusters.&lt;/li&gt;
&lt;li&gt;The wild cluster bootstrap &lt;strong&gt;and&lt;/strong&gt; cluster robust variance estimator with Satterthwaite correction perform astonishingly well for 3 or more clusters. For a number of cluster with of &lt;span class=&#34;math inline&#34;&gt;\(3 \leq G \leq 10\)&lt;/span&gt;, the Satterthwaite correction seems to perform slightly better than the wild cluster bootstrap, which very mildly under-rejects.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation-2-wildly-different-cluster-sizes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Simulation 2: Wildly different cluster sizes&lt;/h4&gt;
&lt;p&gt;Instead of simulating balanced cluster sizes, I now follow &lt;a href=&#34;https://ageconsearch.umn.edu/record/274639/files/qed_wp_1314.pdf&#34;&gt;MacKinnon &amp;amp; Webb&lt;/a&gt; and simulate group sizes that mimic the relative size of the US states (minus Washington DC) for &lt;span class=&#34;math inline&#34;&gt;\(G=50\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(G = 100\)&lt;/span&gt; clusters. The dgp is unchanged, but in parallel to MacKinnon &amp;amp; Webb‚Äôs work, I set the number of observations &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(2.000\)&lt;/span&gt;. I also increase the number of Monte Carlo simulations to &lt;code&gt;n_sim = 5.000&lt;/code&gt; and repeat the analysis for a range of intra-cluster correlations &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:Figure3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;wildly_different.png&#34; alt=&#34;Simulation results. N = 2000, 50 and 100 clusters, wildly different cluster sizes&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Simulation results. N = 2000, 50 and 100 clusters, wildly different cluster sizes
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Both for &lt;span class=&#34;math inline&#34;&gt;\(G = 50\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(G = 100\)&lt;/span&gt;, the wild cluster bootstrap and Satterthwaite corrected errors perform equally well and achieve close to 95% coverage for all intra-cluster correlations &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. The unadjusted CRVE estimates instead tend to under-reject.&lt;/p&gt;
&lt;p&gt;You can reproduce Figure 2 by running&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wildly_different_sim(n = 2000, n_sims = 5000, workers = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once again, note that this function will run for a very long time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;treatment-effects&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Treatment Effects&lt;/h4&gt;
&lt;p&gt;The last simulation investigates a topic that has received considerable attention: regression models where the treatment occurs at the cluster level, but only few clusters are treated (e.g.¬†&lt;a href=&#34;https://academic.oup.com/qje/article-abstract/119/1/249/1876068?redirectedFrom=fulltext&amp;amp;login=false&#34;&gt;‚ÄúHow much should we trust DiD estimates?&lt;/a&gt;‚Äú)? For the sake of simplicity, I do not simulate a‚Äùfull‚Äù DiD model with 2-way fixed effects and potential error correlations across time but restrict myself to replacing &lt;span class=&#34;math inline&#34;&gt;\(X_{ig}\)&lt;/span&gt; in the model above by a treatment assignment dummy &lt;span class=&#34;math inline&#34;&gt;\(D_{ig}\)&lt;/span&gt;. (Loosely) following MacKinnon &amp;amp; Webb once again, I then simulate &lt;span class=&#34;math inline&#34;&gt;\(N=2000\)&lt;/span&gt; observations with intra-cluster correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho = 0.5\)&lt;/span&gt; and vary the number of clusters that are treated. The simulations are repeated for different treated proportions of clusters &lt;span class=&#34;math inline&#34;&gt;\(P \in \{1/50, ..., 1\}\)&lt;/span&gt;, where the clusters are a) of equal size, b) US-state sized and sorted in increasing order and c) US-state sized and sorted in decreasing order .&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;treatment_effect_sim(n = 2000, n_sims = 1000, workers = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:Figure4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;treatment_simulations.png&#34; alt=&#34;Treatment Effect Simulations, N = 2000, 50 clusters, treatment effects. The x axis denotes the share of treated clusters, either in increasing or decreasing cluster size.&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Treatment Effect Simulations, N = 2000, 50 clusters, treatment effects. The x axis denotes the share of treated clusters, either in increasing or decreasing cluster size.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Once again, there is no visible difference in performance between bias-corrected standard errors and the wild cluster bootstrap, but the CRVE tends to overreject in all three scenarios.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;So, when should you use Satterthwaite corrected cluster robust standard errors, and when should you rely on the wild cluster bootstrap in case you are facing a small number of clusters problem or data with wildly different cluster sizes? The honest answer is that I still don‚Äôt know. My main learning from the simulations above is that the Satterthwaite correction method might perform just as well as the wild cluster bootstrap.&lt;/p&gt;
&lt;p&gt;Overall, I am quite impressed by the performance of the Satterthwaite corrected cluster robust sandwich estimator!&lt;/p&gt;
&lt;!-- ### When the parameter of interest is a treatment effect  --&gt;
&lt;!-- In a final simulation, I investigate the (probably) most popular regression specification in all of economics: the regression specification of a Difference-in-Differences strategy - a two-way fixed effects model with a binary variable $D$, a treatment effect.   --&gt;
&lt;!-- Difference-in-Differences models are usually estimated by an  equation similar to  --&gt;
&lt;!-- $$ --&gt;
&lt;!--   y_{igt} =  --&gt;
&lt;!-- $$ --&gt;
&lt;!-- The parameter of interest ... But what happens if only few clusters are treated? Dangers of the pairs bootstrap - bootstrap samples without any treatment group. Solution wild cluster bootstrap. The paper by MacKinnon &amp; Webb argues that the wild cluster bootstrap performs really well if few clusters are treated. Here, I will replicate MW&#39;s analysis for the WCB and compare it with the performance of Satterthwaite corrected robust estimators.  --&gt;
&lt;!-- Again, the data simulating process mimics the equation above. I further set $G = 50$ and the intra-cluster correlation $\rho = $ as in MW. Once again, the sample size is $N = 2000$, and the ...  --&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;You can find all code to reproduce the analyses above in this &lt;a href=&#34;https://github.com/s3alfisc/clusteredErrorsSims&#34;&gt;github repo&lt;/a&gt;. It‚Äôs essentially a clone of code written by Gord Burtch - my estimate for a lower bound of the share of Gordon‚Äôs code is 80%. You can find his code &lt;a href=&#34;https://github.com/gburtch/simulating_cluster_SEs&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;literature&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Literature&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nber.org/system/files/working_papers/t0344/t0344.pdf&#34;&gt;Cameron, Gelbach &amp;amp; Miller - ‚ÄúBootstrap-based improvements for inference with clustered errors‚Äù, Review of Economics &amp;amp; Statistics (2008)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nber.org/system/files/working_papers/w18478/w18478.pdf&#34;&gt;Imbens &amp;amp; Kolesar - ‚ÄúRobust standard errors in small samples: Some practical advice‚Äù, Review of Economics and Statistics (2016)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ageconsearch.umn.edu/record/274639/files/qed_wp_1314.pdf&#34;&gt;MacKinnon &amp;amp; Webb - ‚ÄúWild bootstrap inference for wildly different cluster sizes‚Äù, Journal of Applied Econometrics (2017)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1601.01981.pdf&#34;&gt;Pustejovsky &amp;amp; Tipton - ‚ÄúSmall-sample methods for cluster-robust variance estimation and hypothesis testing in fixed effects models‚Äù, Journal of Economics and Business Statistics (2018)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.econstor.eu/bitstream/10419/97480/1/757403891.pdf&#34;&gt;Webb - ‚ÄúReworking wild bootstrap based inference for clustered errors‚Äù, Working Paper (2013)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>    </item>
    
    <item>
      <title>Hosting a blogdown blog on github pages</title>
      <link>https://s3alfisc.github.io/blog/post/2022-01-24-hosting-a-blogdown-blog-on-github-pages/</link>
      <pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://s3alfisc.github.io/blog/post/2022-01-24-hosting-a-blogdown-blog-on-github-pages/</guid>
   <description>
&lt;script src=&#34;https://s3alfisc.github.io/blog/post/2022-01-24-hosting-a-blogdown-blog-on-github-pages/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I have recently moved from building this blog via the &lt;a href=&#34;https://github.com/rstudio/distill&#34;&gt;distill&lt;/a&gt; package to &lt;a href=&#34;https://github.com/rstudio/blogdown&#34;&gt;blogdown&lt;/a&gt;. The main reason for this is that, at the time of writing, distill does not support &lt;a href=&#34;https://github.com/rstudio/distill/issues/33&#34;&gt;full RSS feeds for multiple articles&lt;/a&gt;, which is a requirement for linking my blog to &lt;a href=&#34;r-bloggers.com&#34;&gt;R-bloggers&lt;/a&gt;. Deploying the &lt;code&gt;distill&lt;/code&gt; based blog via github pages was quite straightforward, but doing so for the &lt;code&gt;blogdown&lt;/code&gt; based blog proved to be slightly more cumbersome.&lt;/p&gt;
&lt;p&gt;While there are many good blog posts on deploying blogdown-blogs on github pages (e.g.¬†&lt;a href=&#34;https://www.r-bloggers.com/2019/09/start-blogging-in-5-minutes-on-netlify-with-hugo-and-blogdown-september-2019-update/&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;https://www.caitlincasar.com/post/blogdown/&#34;&gt;here&lt;/a&gt;), I appear not to be the only one who ran into problems, as this &lt;a href=&#34;https://stackoverflow.com/questions/45362628/github-pages-site-not-detecting-index-html&#34;&gt;stackoverflow thread&lt;/a&gt; with 100+ upvotes shows.&lt;/p&gt;
&lt;p&gt;In my case, I needed to do two things for successful deployment on github pages:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Add a &lt;code&gt;.nojekyll&lt;/code&gt; file to the main directory of the blog, e.g.¬†by running &lt;code&gt;file.create(&#34;.nojekyll&#34;)&lt;/code&gt; in the r console&lt;/li&gt;
&lt;li&gt;Add a &lt;code&gt;publishDir: docs&lt;/code&gt; statement below &lt;code&gt;baseurl&lt;/code&gt; in the &lt;code&gt;config.yaml&lt;/code&gt; file. Rebuilding via &lt;code&gt;blogdown::build_site()&lt;/code&gt; then creates a docs folder and populates it with html. On github pages, I then needed to make sure that the blog is build based on this docs folder (see the image below). After that, I simply had to commit, push, and github actions would finally deploy the blog without any error messages!&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:pressure&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;github_pages.PNG&#34; alt=&#34;Build the github pages site from the &#39;docs&#39; folder&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Build the github pages site from the ‚Äòdocs‚Äô folder
&lt;/p&gt;
&lt;/div&gt;
</description>    </item>
    
    <item>
      <title>Testing an R package against a Julia package on github actions</title>
      <link>https://s3alfisc.github.io/blog/post/testing-an-r-package-against-a-julia-package-via-github-actions/</link>
      <pubDate>Sat, 18 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://s3alfisc.github.io/blog/post/testing-an-r-package-against-a-julia-package-via-github-actions/</guid>
   <description>
&lt;script src=&#34;https://s3alfisc.github.io/blog/post/testing-an-r-package-against-a-julia-package-via-github-actions/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Something I have learned the hard way: every single line of code that I write should be accompanied by a unit test. Unit tests are not only useful for me as a developer to spot bugs, but a thorough sequence of unit tests should also increase others‚Äô confidence in the quality of my code.&lt;/p&gt;
&lt;p&gt;For statistical methods and algorithms, one great way to test if everything is working as intended is to test one‚Äôs own implementation of an algorithm against someone else‚Äôs.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:pressure&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;Caravaggio_Sacrifice_of_Isaac.jpg&#34; alt=&#34;Abraham is *severely* tested. (Sacrifice of Isaac, Caravaggio, ca. 1598. Barbara Piasecka-Johnson Collection.)&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Abraham is &lt;em&gt;severely&lt;/em&gt; tested. (Sacrifice of Isaac, Caravaggio, ca. 1598. Barbara Piasecka-Johnson Collection.)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This idea is expressed in &lt;a href=&#34;https://stats-devguide.ropensci.org/standards.html&#34;&gt;ropensci‚Äôs statistical software standards&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;G5.4 Correctness tests to test that statistical algorithms produce expected results to some fixed test data sets (potentially through comparisons using binding frameworks such as RStata).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;G5.4b For new implementations of existing methods, correctness tests should include tests against previous implementations. Such testing may explicitly call those implementations in testing, preferably from fixed-versions of other software, or use stored outputs from those where that is not possible.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Preferably, these unit tests should be automized via continuous integration tools such as github actions. CI is super useful - a CI workflow might e.g.¬†trigger to run all tests at every commit, so I &lt;em&gt;definitely&lt;/em&gt; won‚Äôt forget to run them. Second, CI integration via github actions communicates to my package‚Äôs user that I have actually and successfully run all unit tests on the latest development version. Last, all tests are run ‚Äúin the cloud‚Äù, so I am not blocking my own computer for (potentially) multiple minutes.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As expressed in ropensci‚Äôs guideline G5.4b, in scientific computing, the same algorithm might only be implemented just once, or, in the lucky case that another implementation exists, it might be written in another language.&lt;/p&gt;
&lt;p&gt;This applies to the ‚Äúfast‚Äù cluster bootstrap algorithm implemented in &lt;a href=&#34;https://github.com/s3alfisc/fwildclusterboot&#34;&gt;fwildclusterboot&lt;/a&gt;. To my knowledge, there exists an R implementation (&lt;code&gt;fwildclusterboot&lt;/code&gt;), the original Stata version (&lt;a href=&#34;https://github.com/droodman/boottest&#34;&gt;boottest&lt;/a&gt;), and recently, David Roodman has written a Julia package, &lt;a href=&#34;https://github.com/droodman/WildBootTests.jl&#34;&gt;WildBootTests.jl&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Up until now, &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; test sequence has been divided in two separate parts. Part one‚Äôs job was to test if the methods for different regression packages in R producec consistent results (‚Äúinternal consistency‚Äù). These tests could easily be run on CRAN and github actions, as they only required &lt;code&gt;fwildclusterboot&lt;/code&gt; and a working R distribution.&lt;/p&gt;
&lt;p&gt;Part two of the test sequence tested &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; ‚Äúexternal validity‚Äù by running the bootstrap in both R and Stata (via &lt;a href=&#34;https://github.com/droodman/boottest&#34;&gt;boottest&lt;/a&gt; &amp;amp; the RStata package). Before a CRAN submission, I would run all these tests on my local machine. Of course, the unit tests involving Stata could not be run on CRAN, and I did not think (and am now uncertain) that these tests could be automatized via github actions.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As it happend, my Stata student license expired the other week, and when I checked StataCorps website for renewal fees, Stata - for non-academics - had a &lt;a href=&#34;https://dpc-onlineshop.de/epages/1188aeca-9275-49f6-be7a-ac117311e6ae.sf/de_DE/?ObjectPath=/Shops/1188aeca-9275-49f6-be7a-ac117311e6ae/Products/5170113&#34;&gt;840 ‚Ç¨ price tag for a one-year license&lt;/a&gt;. To me, this seemed to be a hefty price tag for running a few unit tests for &lt;code&gt;fwildclusterboot&lt;/code&gt;, and was a good incentive to update my unit tests.&lt;/p&gt;
&lt;p&gt;Luckily, &lt;code&gt;WildBootTests.jl&lt;/code&gt; is now around, and as Julia is open source, I was fairly certain that I could transfer my ‚Äúexternal‚Äù tests from Stata to Julia and deploy all tests on github actions.&lt;/p&gt;
&lt;div id=&#34;how-to-set-up-a-workflow-that-tests-an-r-package-against-a-julia-package&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to set up a workflow that tests an R package against a Julia package&lt;/h3&gt;
&lt;p&gt;So, how does a workflow to test an R package against a Julia package looks like?&lt;/p&gt;
&lt;p&gt;First of all, I have used the skeleton of &lt;code&gt;fwildclusterboot&lt;/code&gt; to produce a wrapper around &lt;code&gt;WildBootTests.jl&lt;/code&gt;, &lt;a href=&#34;https://github.com/s3alfisc/wildboottestjlr&#34;&gt;wildboottestjlr&lt;/a&gt;. All data pre-processing is handled by functions copied from &lt;code&gt;fwildclusterboot&lt;/code&gt;. For running the bootstrap, R sends all required objects to Julia (via the excellent &lt;code&gt;JuliaConnectoR&lt;/code&gt; package, on which I will blog in the future). In a second step, I have added &lt;code&gt;wildboottestjlr&lt;/code&gt; to &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; dependencies and have initiated a github actions workflow via &lt;code&gt;usethis::use_github_actions()&lt;/code&gt;. (Note that it is in fact not neccessariy to write an entire R wrapper-package for the Julia algorithm you want to test. &lt;code&gt;wildboottestjlr&lt;/code&gt; exists because &lt;code&gt;WildBootTests.jl&lt;/code&gt; is actually a magnitude faster than &lt;code&gt;fwildclusterboot&lt;/code&gt; and because it has many additional feature, as e.g.¬†a wild cluster bootstrap for instrumental variables.)&lt;/p&gt;
&lt;p&gt;The created workflow yaml looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For help debugging build failures open an issue on the RStudio community with the &amp;#39;github-actions&amp;#39; tag.
# https://community.rstudio.com/new-topic?category=Package%20development&amp;amp;tags=github-actions
on:
  push:
    branches:
      - main
      - master
  pull_request:
    branches:
      - main
      - master

name: R-CMD-check

jobs:
  R-CMD-check:
    runs-on: ${{ matrix.config.os }}

    name: ${{ matrix.config.os }} (${{ matrix.config.r }})

    strategy:
      fail-fast: false
      matrix:
        config:
          - {os: windows-latest, r: &amp;#39;release&amp;#39;}
          - {os: macOS-latest, r: &amp;#39;release&amp;#39;}
          - {os: ubuntu-20.04, r: &amp;#39;release&amp;#39;, rspm: &amp;quot;https://packagemanager.rstudio.com/cran/__linux__/focal/latest&amp;quot;}

    env:
      R_REMOTES_NO_ERRORS_FROM_WARNINGS: true
      RSPM: ${{ matrix.config.rspm }}
      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}

    steps:
      - uses: actions/checkout@v2

      - uses: r-lib/actions/setup-r@v2
        with:
          r-version: ${{ matrix.config.r }}

      - uses: r-lib/actions/setup-pandoc@v2

      - name: Query dependencies
        run: |
          install.packages(&amp;#39;remotes&amp;#39;)
          saveRDS(remotes::dev_package_deps(dependencies = TRUE), &amp;quot;.github/depends.Rds&amp;quot;, version = 2)
          writeLines(sprintf(&amp;quot;R-%i.%i&amp;quot;, getRversion()$major, getRversion()$minor), &amp;quot;.github/R-version&amp;quot;)
        shell: Rscript {0}

      - name: Cache R packages
        if: runner.os != &amp;#39;Windows&amp;#39;
        uses: actions/cache@v2
        with:
          path: ${{ env.R_LIBS_USER }}
          key: ${{ runner.os }}-${{ hashFiles(&amp;#39;.github/R-version&amp;#39;) }}-1-${{ hashFiles(&amp;#39;.github/depends.Rds&amp;#39;) }}
          restore-keys: ${{ runner.os }}-${{ hashFiles(&amp;#39;.github/R-version&amp;#39;) }}-1-

      - name: Install system dependencies
        if: runner.os == &amp;#39;Linux&amp;#39;
        run: |
          while read -r cmd
          do
            eval sudo $cmd
          done &amp;lt; &amp;lt;(Rscript -e &amp;#39;writeLines(remotes::system_requirements(&amp;quot;ubuntu&amp;quot;, &amp;quot;20.04&amp;quot;))&amp;#39;)
      - name: Install dependencies
        run: |
          remotes::install_deps(dependencies = TRUE)
          remotes::install_cran(&amp;quot;rcmdcheck&amp;quot;)
        shell: Rscript {0}
        
      # install julia
      - uses: julia-actions/setup-julia@v1
      # add julia to renviron
      - name: Create and populate .Renviron file
        run: echo JULIA_BINDIR= &amp;quot;${{ env.juliaLocation }}&amp;quot; &amp;gt;&amp;gt; ~/.Renviron
        shell: bash

      # install WildBootTests.jl
      - name: install WildBootTests.jl 
        run: julia -e &amp;#39;using Pkg; Pkg.add(&amp;quot;WildBootTests&amp;quot;)&amp;#39;
        # use shell bash to ensure consistent behavior across OS
        shell: bash

      - name: Check
        env:
          _R_CHECK_CRAN_INCOMING_REMOTE_: false
        run: rcmdcheck::rcmdcheck(args = c(&amp;quot;--no-manual&amp;quot;, &amp;quot;--as-cran&amp;quot;), error_on = &amp;quot;warning&amp;quot;, check_dir = &amp;quot;check&amp;quot;)
        shell: Rscript {0}

      - name: Upload check results
        if: failure()
        uses: actions/upload-artifact@main
        with:
          name: ${{ runner.os }}-r${{ matrix.config.r }}-results
          path: check&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a nutshell, this workflow installs R and &lt;code&gt;fwildclusterboot&lt;/code&gt; in a clean ubuntu environment, conducts a cmd-check and initiates all of &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; unit tests.&lt;/p&gt;
&lt;p&gt;To enable testing against Julia and &lt;code&gt;WildBootTests.jl&lt;/code&gt;, it further adds three ‚Äòsteps‚Äô to the workflow: the workflow has to install &lt;code&gt;Julia&lt;/code&gt;, &lt;code&gt;WildBootTests.jl&lt;/code&gt; (and all its Julia dependencies) and link R and Julia so that &lt;code&gt;JuliaConnectoR&lt;/code&gt; could communicate between the two languages.&lt;/p&gt;
&lt;p&gt;This is easily be achieved by adding the lines below to the yaml file create by &lt;code&gt;usethis::use_github_action()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install julia
- uses: julia-actions/setup-julia@v1
# add julia to renviron
- name: Create and populate .Renviron file
  run: echo JULIA_BINDIR= &amp;quot;${{ env.juliaLocation }}&amp;quot; &amp;gt;&amp;gt; ~/.Renviron
  shell: bash

# install WildBootTests.jl
  - name: install WildBootTests.jl 
    run: julia -e &amp;#39;using Pkg; Pkg.add(&amp;quot;WildBootTests&amp;quot;)&amp;#39;
  # use shell bash to ensure consistent behavior across OS
    shell: bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Placing these few lines in the workflow yaml before the cmd check is triggered completes the workflow. It now downloads R and Julia, installs &lt;code&gt;fwildclusterboot&lt;/code&gt;, &lt;code&gt;wildboottestjlr&lt;/code&gt; and &lt;code&gt;WildBootTests.jl&lt;/code&gt;, links R and Julia, and runs all of &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; unit tests. The remaining step is to push to github, and then all unit tests are running!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The unit tests for fwildclusterboot currently run for about 30 minutes.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I have not implemented test that compare &lt;code&gt;fwildclusterboot&lt;/code&gt; against other R implementations of the wild bootstrap, mainly due to performance reasons. As resampling methods are usually time intensive and fwildclusterboot is &lt;a href=&#34;https://s3alfisc.github.io/post/1000x-faster-wild-cluster-bootstrap-inference-in-r-with-fwildclusterboot/&#34;&gt;much faster&lt;/a&gt; than other R implementations, running unit tests against other R implementations would simply take a &lt;em&gt;very&lt;/em&gt; long time.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>    </item>
    
    <item>
      <title>1000x faster Wild Cluster Bootstrap Inference in R with fwildclusterboot üöÄ</title>
      <link>https://s3alfisc.github.io/blog/post/1000x-faster-wild-cluster-bootstrap-inference-in-r-with-fwildclusterboot/</link>
      <pubDate>Sun, 12 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://s3alfisc.github.io/blog/post/1000x-faster-wild-cluster-bootstrap-inference-in-r-with-fwildclusterboot/</guid>
   <description>
&lt;script src=&#34;https://s3alfisc.github.io/blog/post/1000x-faster-wild-cluster-bootstrap-inference-in-r-with-fwildclusterboot/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;When you suspect that the error terms in your regression model are correlated within clusters, and the number of clusters is small, trouble might be running at you. In such a situation, common cluster robust standard errors tend to be downward biased - they are too eager to reject the null hypothesis. Since &lt;a href=&#34;https://www.jstor.org/stable/40043157&#34;&gt;Cameron, Gelbach &amp;amp; Miller&lt;/a&gt; first suggested that the wild cluster bootstrap might be preferable to sandwich standard errors when the number of clusters is small, it has become common practice among empirical economists to check their cluster robust inferences against the wild cluster bootstrap.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:pressure&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;duerer_lion.jpg&#34; alt=&#34;Not a wild bootstrap, but a wild lion, by Albrecht Duerer&#34; width=&#34;75%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Not a wild bootstrap, but a wild lion, by Albrecht Duerer
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;At some point, I found myself in a ‚Äúsmall number of clusters‚Äù situation. I was trying to estimate a treatment effect for a sample of a few thousand observations, which were grouped into around 20 clusters. So I started to search for R packages that implement the wild cluster bootstrap, and found two implementations on CRAN: &lt;code&gt;sandwich&lt;/code&gt; and &lt;code&gt;clusterSEs&lt;/code&gt;. I opted for the &lt;code&gt;sandwich&lt;/code&gt; package (because it‚Äôs actually a really great package) and fit my regression model via &lt;code&gt;lm()&lt;/code&gt;. Then I started to bootstrap with sandwich‚Äôs &lt;code&gt;vcovBS()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;So the bootstrap ran ‚Ä¶ and I waited. Eventually, I left my office to get some coffee with a colleague, returned to my desk ‚Ä¶ and the bootstrap still ran, and I waited even longer.&lt;/p&gt;
&lt;p&gt;But while the bootstrap was running, I scrolled the web and stumbled over the &lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/1536867X19830877?journalCode=stja&#34;&gt;‚ÄúFast &amp;amp; Wild‚Äù paper&lt;/a&gt; by Roodman et al (2019). The claimed performance in the paper seemed to good to be true: bootstrap inference with several thousands of iterations, in a fraction of a second? The paper presents a Stata implementation of the fast algorithm, &lt;a href=&#34;https://github.com/droodman/bottest&#34;&gt;boottest&lt;/a&gt;, and that was a good enough reason for me to start up a Stata session to try it out.&lt;/p&gt;
&lt;p&gt;And indeed, &lt;code&gt;boottest&lt;/code&gt; &lt;em&gt;is&lt;/em&gt; mind-blowingly fast: the bootstrap finished almost instantaneously. I was hooked: how was it possible that &lt;code&gt;boottest&lt;/code&gt; was &lt;em&gt;so damn fast&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;Luckily, the ‚ÄúFast &amp;amp; Wild‚Äù paper explains the algorithm powering &lt;code&gt;boottest&lt;/code&gt; in great detail. Out of curiosity, I started to implement it in R, and the &lt;code&gt;fwildclusterboot&lt;/code&gt; package is the result of this effort. Now, was it worth all the work? How much faster is the ‚Äúfast algorithm‚Äù implemented in &lt;code&gt;fwildclusterboot&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;To compare &lt;code&gt;fwildclusterboot&#39;s&lt;/code&gt; performance to &lt;code&gt;sandwich&lt;/code&gt;, I simulate a data set with &lt;span class=&#34;math inline&#34;&gt;\(N = 10.000\)&lt;/span&gt; observations and &lt;span class=&#34;math inline&#34;&gt;\(N_G = 42\)&lt;/span&gt; distinct clusters (42 is the magic number of clusters for which the economics profession has decided that large N asymptotics fail, see Angrist &amp;amp; Pischke‚Äôs ‚ÄúMostly Harmless‚Äù, Chapter 8.2.3) and fit a regression model via &lt;code&gt;lm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fwildclusterboot)
library(sandwich)
library(lmtest)
library(bench)

# simulate data
seed &amp;lt;- 236723478
N &amp;lt;- 10000
data &amp;lt;- fwildclusterboot:::create_data(N = N,
                                         N_G1 = 42, icc1 = 0.1,
                                         N_G2 = 20, icc2 = 0.8,
                                         numb_fe1 = 10,
                                         numb_fe2 = 10,
                                         seed = seed,
                                         weights = 1:N)
lm_fit &amp;lt;- lm(proposition_vote ~ treatment + as.factor(Q1_immigration) + as.factor(Q2_defense), data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the first experiment, the bootstrap will run for &lt;span class=&#34;math inline&#34;&gt;\(B = 9999\)&lt;/span&gt; iterations. For the estimation via &lt;code&gt;vcovBS&lt;/code&gt;, we will use 4 cores.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 9999
# wild cluster bootstrap via sandwich::vcovBS

bench1 &amp;lt;- 
bench::mark(
  boot_slow = sandwich::vcovBS(lm_fit,
                                R = B,
                                cluster = ~ group_id1,
                                cores = 4), 
  iterations = 1
)
bench1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##   expression      min   median `itr/sec` mem_alloc `gc/sec`
##   &amp;lt;bch:expr&amp;gt; &amp;lt;bch:tm&amp;gt; &amp;lt;bch:tm&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;bch:byt&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 boot_slow     36.9s    36.9s    0.0271    12.7MB        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;vcovBS()&lt;/code&gt; finishes in around 37 seconds - that‚Äôs not too bad, isn‚Äôt it?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# wild cluster bootstrap via fwildclusterboot::boottest()
bench1f &amp;lt;- 
bench::mark(boot_fast =
                   fwildclusterboot::boottest(lm_fit,
                                              clustid = c(&amp;quot;group_id1&amp;quot;),
                                              B = B,
                                              param = &amp;quot;treatment&amp;quot;,
                                              seed = 3,
                                              nthreads = 1), 
            iterations = 25)
bench1f&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##   expression      min   median `itr/sec` mem_alloc `gc/sec`
##   &amp;lt;bch:expr&amp;gt; &amp;lt;bch:tm&amp;gt; &amp;lt;bch:tm&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;bch:byt&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 boot_fast    73.3ms   81.7ms      9.48    98.7MB     26.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While &lt;code&gt;sandwich::vcovBS()&lt;/code&gt; takes almost 36.9 seconds, &lt;code&gt;fwildclusterboot::boottest()&lt;/code&gt; runs in around one fifth of a second üöÄ. Yes, really: one fifth of a second! That‚Äôs a speed gain of a factor of 451! If you don‚Äôt have 4 cores available, performance differences get even more extreme (e.g.¬†if you only have one core, you have to multiply 37 with a number slightly smaller than 4).&lt;/p&gt;
&lt;p&gt;How do &lt;code&gt;vcovBS()&#39;s&lt;/code&gt; and &lt;code&gt;boottest()&#39;s&lt;/code&gt; results compare?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(boot_fast)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## boottest.lm(object = lm_fit, clustid = c(&amp;quot;group_id1&amp;quot;), param = &amp;quot;treatment&amp;quot;, 
##     B = B, seed = 3, nthreads = 1)
##  
##  Hypothesis: 1*treatment = 0
##  Observations: 10000
##  Bootstr. Iter: 9999
##  Bootstr. Type: rademacher
##  Clustering: 1-way
##  Confidence Sets: 95%
##  Number of Clusters: 42
## &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              term estimate statistic p.value conf.low conf.high
## 1 1*treatment = 0    0.002     0.516   0.605   -0.007     0.012&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmtest::coeftest(x = lm_fit, vcov = boot_slow)[2,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Estimate  Std. Error     t value    Pr(&amp;gt;|t|) 
## 0.002387792 0.004571759 0.522291836 0.601478745&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmtest::coefci(x = lm_fit, vcov = boot_slow)[2,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        2.5 %       97.5 % 
## -0.006573777  0.011349362&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Between the two implementations, the bootstrapped t-statistics, p-values and confidence intervals are almost identical. They are not exactly identical for two reasons: first due to sampling uncertainty in the bootstrap, and second because &lt;code&gt;vcovBS&lt;/code&gt; does not apply any small sample adjustments (at least I could not find anything related to small-sample adjustments in both documentation and source code).&lt;/p&gt;
&lt;p&gt;The speed gains of &lt;code&gt;fwildclusterboot&lt;/code&gt; scale well in the number of bootstrap iterations. For &lt;span class=&#34;math inline&#34;&gt;\(B = 99.999\)&lt;/span&gt; iterations, it finishes in around one second. For &lt;code&gt;vcovBS&lt;/code&gt;, you can expect a linear increase in run-time in the number of bootstrap iterations: a ten-fold increase in bootstrap iterations will increase run-time to around 360 seconds.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 99999

bench2f &amp;lt;- 
bench::mark(
  boot_fast =
    fwildclusterboot::boottest(lm_fit,
                             clustid = c(&amp;quot;group_id1&amp;quot;),
                             B = B,
                             param = &amp;quot;treatment&amp;quot;,
                             seed = 3,
                             nthreads = 1), 
  iterations = 10
)

bench2f&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##   expression      min   median `itr/sec` mem_alloc `gc/sec`
##   &amp;lt;bch:expr&amp;gt; &amp;lt;bch:tm&amp;gt; &amp;lt;bch:tm&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;bch:byt&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 boot_fast     476ms    571ms      1.72     727MB     11.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happens if we increase the sample size to &lt;span class=&#34;math inline&#34;&gt;\(N = 100.000\)&lt;/span&gt;?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N &amp;lt;- 100000
data &amp;lt;- fwildclusterboot:::create_data(N = N,
                                         N_G1 = 50, icc1 = 0.1,
                                         N_G2 = 20, icc2 = 0.8,
                                         numb_fe1 = 10,
                                         numb_fe2 = 10,
                                         seed = seed,
                                         weights = 1:N)
lm_fit &amp;lt;- lm(proposition_vote ~ treatment + as.factor(Q1_immigration) + as.factor(Q2_defense), data)
B &amp;lt;- 9999
# wild cluster bootstrap via sandwich::vcovBS
bench3 &amp;lt;- bench::mark(
  boot_slow = sandwich::vcovBS(lm_fit,
                                R = B,
                                cluster = ~ group_id1,
                                cores = 4), 
  iterations = 1)
bench3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##   expression      min   median `itr/sec` mem_alloc `gc/sec`
##   &amp;lt;bch:expr&amp;gt; &amp;lt;bch:tm&amp;gt; &amp;lt;bch:tm&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;bch:byt&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 boot_slow     8.32m    8.32m   0.00200    31.2MB        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More than 8 minutes pass before &lt;code&gt;vcovBS()&lt;/code&gt; finishes. How does &lt;code&gt;boottest()&lt;/code&gt; do in comparison?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# wild cluster bootstrap via fwildclusterboot::boottest()

bench3f &amp;lt;- 
bench::mark(
  boot_fast =
    fwildclusterboot::boottest(lm_fit,
                             clustid = c(&amp;quot;group_id1&amp;quot;),
                             B = B,
                             param = &amp;quot;treatment&amp;quot;,
                             seed = 3,
                             nthreads = 1), 
iterations = 5)
bench3f&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##   expression      min   median `itr/sec` mem_alloc `gc/sec`
##   &amp;lt;bch:expr&amp;gt; &amp;lt;bch:tm&amp;gt; &amp;lt;bch:tm&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;bch:byt&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 boot_fast     310ms    333ms      2.68     308MB     10.7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With &lt;code&gt;B = 9999&lt;/code&gt; iterations, &lt;code&gt;boottest()&lt;/code&gt; runs for around 0.33 seconds, while &lt;code&gt;vcovBS()&lt;/code&gt; only finishes after 499.36 seconds. &lt;code&gt;fwildclusterboot::boottest()&lt;/code&gt; is 1499 times faster than &lt;code&gt;sandwich::vcovBS&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;As a conclusion: if you face a ‚Äúsmall number of clusters‚Äù problem and want to reduce your daily ‚òï consumption, you should consider using &lt;a href=&#34;https://github.com/s3alfisc/fwildclusterboot&#34;&gt;fwildclusterboot&lt;/a&gt;, Stata‚Äôs &lt;a href=&#34;https://github.com/droodman/boottest&#34;&gt;boottest&lt;/a&gt;, or &lt;a href=&#34;https://github.com/droodman/WildBootTests.jl&#34;&gt;WildBootTests.jl&lt;/a&gt;, which is a novel Julia implementation of the ‚Äúfast algorithm‚Äù. If all of this seems like black magic to you and you want to learn more about the ‚Äúfast algorithm‚Äù, I cannot recommend the ‚ÄúFast &amp;amp; Wild‚Äù paper highly enough.&lt;/p&gt;
&lt;div id=&#34;literature&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Literature&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;‚ÄúFast &amp;amp; Wild‚Äù, Roodman et al.¬†(2019), The Stata Journal&lt;/li&gt;
&lt;li&gt;‚ÄúBootstrap-Based Improvements for Inference with Clustered Errors‚Äù, Cameron, Gelbach &amp;amp; Miller (2008), The Review of Economics and Statistics&lt;/li&gt;
&lt;li&gt;‚ÄúCluster-robust inference: A guide to empirical practice‚Äù (2020), MacKinnon, Oerregaard Nielsen &amp;amp; Webb, Working Paper&lt;/li&gt;
&lt;li&gt;‚ÄúMostly Harmless Econometrics‚Äù, Angrist &amp;amp; Pischke (2009), Princeton University Press&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>    </item>
    
  </channel>
</rss>
