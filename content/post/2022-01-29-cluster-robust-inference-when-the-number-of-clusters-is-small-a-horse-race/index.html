---
title: 'A üê¥ race: The Wild Cluster Bootstrap vs Satterthwaite-corrected Sandwich Estimators when the number of Clusters is small'
author: 'Alexander Fischer'
date: '2022-01-29'
slug: []
categories: [statistical inference, wild cluster bootstrap, clustered errors, Satterthwaite correction]
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>A couple of months ago, Gordon Burtch shared an excellent Twitter thread on the merits of wild cluster bootstrap inference when the regression error terms are clustered into a small group of clusters:</p>
<p>{{% tweet "1378520203689082886" %}}</p>
<!-- <blockquote class="twitter-tweet"><p lang="en" dir="ltr">How biased are clustered SEs with &#39;few&#39; clusters? A simulation illustrating this. DGP is y~x, 50 clusters, x is normal, true beta is 0.5. Plot of 1000 sims, beta estimate +95% CIs for each. Red = we did not cover true beta. Std SEs no good, clustered SEs yield ~95% coverage (1/6) <a href="https://t.co/z3eZdy1wb1">pic.twitter.com/z3eZdy1wb1</a></p>&mdash; Gord Burtch (@gburtch) <a href="https://twitter.com/gburtch/status/1378520203689082886?ref_src=twsrc%5Etfw">April 4, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>  -->
<p>In his simulation study, Gordon focused on the wild cluster bootstrap vs regular sandwich cluster robust inference (CRVE). As I am quite invested in the wild cluster bootstrap, I was happy to see that it appeared to outperform ‚Äòclassical‚Äô robust standard errors: in his simulations, the coverage rate of wild clustered bootstrapped confidence intervals is already close to the desired coverage rate of 95% even for a small number of clusters (e.g.¬†5-10).</p>
<!-- Have you ever heard of Satterthwaite degree-of-freedom bias-corrections for cluster robust standard error estimation? -->
<!-- Well, I myself have never seen them used in any applied econometrics paper \footnote{As it happens, Angrist \& Lavy ... }. But while writing this blog post, I realized that both the survey paper by [Cameron & Miller](http://cameron.econ.ucdavis.edu/research/Cameron_Miller_JHR_2015_February.pdf) and Mostly Harmless Econometrics both discuss ... -->
<p>Nevertheless, there is a statistical literature that argues that it is fine to use cluster robust sandwich estimators to compute standard errors for a small number of clusters as long as one applies an appropriate <strong>small sample correction</strong> via <strong>Satterthwaite</strong> or <strong>saddlepoint corrections</strong> (Imbens &amp; Kolesar, Bell, Tipton &amp; Pustejovksy).
All these methods are implemented in R via the <code>clubSandwich</code> package and in Stata in the <a href="https://github.com/jepusto/clubSandwich-Stata">clubSandwich-Stata</a> package.</p>
<p>Of course I was curious to see how the Satterthwaite-corrected SEs would perform in comparison to the would cluster bootstrap, so I decided to run some simulations.</p>
<p>Luckily for me, Gordon published all of his code <a href="https://github.com/gburtch/simulating_cluster_SEs">on github</a>, so it was easy for me to slightly tweak it and add simulations for Satterthwaite corrections. Open software is really awesome!</p>
<p>I have collected my minor updates of Gordon‚Äôs code in an R package, which is available on <a href="https://github.com/s3alfisc/clusteredErrorsSims">github</a>. To reproduce all analyses below, you simply have to install the package by running</p>
<pre class="r"><code># install.packages(&quot;devtools&quot;)
devtools::install_github(&quot;s3alfisc/clusteredErrorsSims&quot;)</code></pre>
<p>But before we dive into the simulations, I will start with revising some theory on the consistency of CRVE that will motivate the design of the simulations.</p>
<div id="when-are-clustered-standard-errors-biased" class="section level2">
<h2>When are clustered standard errors biased?</h2>
<p>In general, cluster robust variance estimators will be biased if one of the three conditions below holds:</p>
<ol style="list-style-type: decimal">
<li>If there are only very few clusters.</li>
<li>If the cluster sizes are wildly different.</li>
<li>If the intra-cluster correlations varies across clusters.</li>
</ol>
<p>In the following simulations, I will focus on cases 1-2 and conduct three simulation studies. The <strong>first</strong> simulation closely follows Gordon‚Äôs work and investigates the performances of different inference methods for a <strong>small number of clusters G</strong>, but includes simulations for Satterthwaite corrected CRVE estimates via the <code>clubSandwich</code> package. The <strong>second</strong> set of simulations investigates the performance for clustered erros with <span class="math inline">\(G \in \{50, 100\}\)</span> clusters, but <strong>wildly different</strong> cluster sizes. Last, I take a look at a special case that has received considerable attention: how do wild cluster bootstrap and Satterthwaite corrected SEs perform when only few clusters are treated (as often happens with Difference-in-Differences identification strategies)? Simulations 2 and 3 are heavily influenced by work by <a href="https://ageconsearch.umn.edu/record/274639/files/qed_wp_1314.pdf">MacKinnon &amp; Webb</a> on the performance of the wild cluster bootstrap under ‚Äúwildly different‚Äù cluster sizes.</p>
<p>The data generating process for all simulations is a simple linear regression model for <span class="math inline">\(g = 1, ..., G\)</span> clusters:</p>
<p><span class="math display">\[
  y_{ig} = \beta_0 + \beta_1 X_{ig} + \epsilon_{ig}
\]</span></p>
<p>where <span class="math inline">\(E(\epsilon_{ig}|X_{ig}) = 0\)</span>, <span class="math inline">\(\beta_0 = 1\)</span>, <span class="math inline">\(\beta_1 = 0.5\)</span> and the errors <span class="math inline">\(\epsilon_{ig}\)</span> are simulated to be correlated within <span class="math inline">\(G\)</span> clusters with intra-cluster correlation <span class="math inline">\(\rho\)</span>. All errors are uncorrelated across clusters.</p>
<p>So the stage is set for a üê¥ race! My champion, of course, is the wild cluster bootstrap, but let‚Äôs see how the bias-corrected standard errors perform in comparison!</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pressure"></span>
<img src="Le_jockey.jpg" alt="Toulose-Lautrec, Le Jockey, 1899" width="75%" />
<p class="caption">
Figure 1: Toulose-Lautrec, Le Jockey, 1899
</p>
</div>
<div id="simulation-1-small-number-of-clusters-g-balanced-cluster-sizes" class="section level4">
<h4>Simulation 1: Small number of clusters G &amp; balanced cluster sizes</h4>
<p>To initiate the horse race, you simply have to run the <code>sim_balanced_clusters()</code> function, though I want to note that on my laptop, this takes around 2h while using multiple cores. By default, the wild cluster bootstrap will run with <span class="math inline">\(B = 9999\)</span> bootstrap iterations throughout all simulations.</p>
<pre class="r"><code>library(clusteredErrorsSims)
set.seed(1234)
sim_balanced_clusters(n = 1000, n_sims = 1000, rho = 0.7, workers = 4)</code></pre>
<p>Here are the results for the first simulation:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Result1"></span>
<img src="CI%20Coverage%20for%20Several%20Cluster%20Robust%20Inference%20Methods.png" alt="Simulation results. N = 1000, rho = 0.7, balanced cluster sizes" width="75%" />
<p class="caption">
Figure 2: Simulation results. N = 1000, rho = 0.7, balanced cluster sizes
</p>
</div>
<p>There are three takeaways from figure 2:</p>
<ul>
<li>As expected, inference with non-robust standard errors is severely biased.</li>
<li>Inference via cluster robust estimators tends to under-reject for less than 50 clusters.</li>
<li>The wild cluster bootstrap <strong>and</strong> cluster robust variance estimator with Satterthwaite correction perform astonishingly well for 3 or more clusters. For a number of cluster with of <span class="math inline">\(3 \leq G \leq 10\)</span>, the Satterthwaite correction seems to perform slightly better than the wild cluster bootstrap, which very mildly under-rejects.</li>
</ul>
</div>
<div id="simulation-2-wildly-different-cluster-sizes" class="section level4">
<h4>Simulation 2: Wildly different cluster sizes</h4>
<p>Instead of simulating balanced cluster sizes, I now follow <a href="https://ageconsearch.umn.edu/record/274639/files/qed_wp_1314.pdf">MacKinnon &amp; Webb</a> and simulate group sizes that mimic the relative size of the US states (minus Washington DC) for <span class="math inline">\(G=50\)</span> and <span class="math inline">\(G = 100\)</span> clusters. The dgp is unchanged, but in parallel to MacKinnon &amp; Webb‚Äôs work, I set the number of observations <span class="math inline">\(N\)</span> to <span class="math inline">\(2.000\)</span>. I also increase the number of Monte Carlo simulations to <code>n_sim = 5.000</code> and repeat the analysis for a range of intra-cluster correlations <span class="math inline">\(\rho\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure3"></span>
<img src="wildly_different.png" alt="Simulation results. N = 2000, 50 and 100 clusters, wildly different cluster sizes" width="75%" />
<p class="caption">
Figure 3: Simulation results. N = 2000, 50 and 100 clusters, wildly different cluster sizes
</p>
</div>
<p>Both for <span class="math inline">\(G = 50\)</span> and <span class="math inline">\(G = 100\)</span>, the wild cluster bootstrap and Satterthwaite corrected errors perform equally well and achieve close to 95% coverage for all intra-cluster correlations <span class="math inline">\(\rho\)</span>. The unadjusted CRVE estimates instead tend to under-reject.</p>
<p>You can reproduce Figure 2 by running</p>
<pre class="r"><code>wildly_different_sim(n = 2000, n_sims = 5000, workers = 4)</code></pre>
<p>Once again, note that this function will run for a very long time.</p>
</div>
<div id="treatment-effects" class="section level4">
<h4>Treatment Effects</h4>
<p>The last simulation investigates a topic that has received considerable attention: regression models where the treatment occurs at the cluster level, but only few clusters are treated (e.g.¬†<a href="https://academic.oup.com/qje/article-abstract/119/1/249/1876068?redirectedFrom=fulltext&amp;login=false">‚ÄúHow much should we trust DiD estimates?</a>‚Äú)? For the sake of simplicity, I do not simulate a‚Äùfull‚Äù DiD model with 2-way fixed effects and potential error correlations across time but restrict myself to replacing <span class="math inline">\(X_{ig}\)</span> in the model above by a treatment assignment dummy <span class="math inline">\(D_{ig}\)</span>. (Loosely) following MacKinnon &amp; Webb once again, I then simulate <span class="math inline">\(N=2000\)</span> observations with intra-cluster correlation <span class="math inline">\(\rho = 0.5\)</span> and vary the number of clusters that are treated. The simulations are repeated for different treated proportions of clusters <span class="math inline">\(P \in \{1/50, ..., 1\}\)</span>, where the clusters are a) of equal size, b) US-state sized and sorted in increasing order and c) US-state sized and sorted in decreasing order .</p>
<pre class="r"><code>treatment_effect_sim(n = 2000, n_sims = 1000, workers = 4)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure4"></span>
<img src="treatment_simulations.png" alt="Treatment Effect Simulations, N = 2000, 50 clusters, treatment effects. The x axis denotes the share of treated clusters, either in increasing or decreasing cluster size." width="75%" />
<p class="caption">
Figure 4: Treatment Effect Simulations, N = 2000, 50 clusters, treatment effects. The x axis denotes the share of treated clusters, either in increasing or decreasing cluster size.
</p>
</div>
<p>Once again, there is no visible difference in performance between bias-corrected standard errors and the wild cluster bootstrap, but the CRVE tends to underreject in all three scenarios.</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>So, when should you use Satterthwaite corrected cluster robust standard errors, and when should you rely on the wild cluster bootstrap in case you are facing a small number of clusters problem or data with wildly different cluster sizes? The honest answer is that I still don‚Äôt know. My main learning from the simulations above is that the Satterthwaite correction method might perform just as well as the wild cluster bootstrap.</p>
<p>Overall, I am quite impressed by the performance of the Satterthwaite corrected cluster robust sandwich estimator!</p>
<!-- ### When the parameter of interest is a treatment effect  -->
<!-- In a final simulation, I investigate the (probably) most popular regression specification in all of economics: the regression specification of a Difference-in-Differences strategy - a two-way fixed effects model with a binary variable $D$, a treatment effect.   -->
<!-- Difference-in-Differences models are usually estimated by an  equation similar to  -->
<!-- $$ -->
<!--   y_{igt} =  -->
<!-- $$ -->
<!-- The parameter of interest ... But what happens if only few clusters are treated? Dangers of the pairs bootstrap - bootstrap samples without any treatment group. Solution wild cluster bootstrap. The paper by MacKinnon & Webb argues that the wild cluster bootstrap performs really well if few clusters are treated. Here, I will replicate MW's analysis for the WCB and compare it with the performance of Satterthwaite corrected robust estimators.  -->
<!-- Again, the data simulating process mimics the equation above. I further set $G = 50$ and the intra-cluster correlation $\rho = $ as in MW. Once again, the sample size is $N = 2000$, and the ...  -->
</div>
<div id="code" class="section level2">
<h2>Code</h2>
<p>You can find all code to reproduce the analyses above in this <a href="https://github.com/s3alfisc/clusteredErrorsSims">github repo</a>. It‚Äôs essentially a clone of code written by Gord Burtch - my estimate for a lower bound of the share of Gordon‚Äôs code is 80%. You can find his code <a href="https://github.com/gburtch/simulating_cluster_SEs">here</a>.</p>
</div>
<div id="literature" class="section level2">
<h2>Literature</h2>
<ul>
<li><a href="https://www.nber.org/system/files/working_papers/t0344/t0344.pdf">Cameron, Gelbach &amp; Miller - ‚ÄúBootstrap-based improvements for inference with clustered errors‚Äù, Review of Economics &amp; Statistics (2008)</a></li>
<li><a href="https://www.nber.org/system/files/working_papers/w18478/w18478.pdf">Imbens &amp; Kolesar - ‚ÄúRobust standard errors in small samples: Some practical advice‚Äù, Review of Economics and Statistics (2016)</a></li>
<li><a href="https://ageconsearch.umn.edu/record/274639/files/qed_wp_1314.pdf">MacKinnon &amp; Webb - ‚ÄúWild bootstrap inference for wildly different cluster sizes‚Äù, Journal of Applied Econometrics (2017)</a></li>
<li><a href="https://arxiv.org/pdf/1601.01981.pdf">Pustejovsky &amp; Tipton - ‚ÄúSmall-sample methods for cluster-robust variance estimation and hypothesis testing in fixed effects models‚Äù, Journal of Economics and Business Statistics (2018)</a></li>
<li><a href="https://www.econstor.eu/bitstream/10419/97480/1/757403891.pdf">Webb - ‚ÄúReworking wild bootstrap based inference for clustered errors‚Äù, Working Paper (2013)</a></li>
</ul>
</div>
